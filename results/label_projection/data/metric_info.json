[
    {
        "metric_name": "Accuracy",
        "metric_summary": "Average number of correctly applied labels.",
        "paper_reference": "grandini2020metrics",
        "maximize": true,
        "image": "openproblems",
        "task_id": "label_projection",
        "commit_sha": "b3456fd73c04c28516f6df34c57e6e3e8b0dab32",
        "metric_id": "accuracy",
        "implementation_url": "https://github.com/openproblems-bio/openproblems/blob/main/openproblems/tasks/label_projection/metrics/accuracy.py"
    },
    {
        "metric_name": "F1 score",
        "metric_summary": "The [F1 score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) is a weighted average of the precision and recall over all class labels, where an F1 score reaches its best value at 1 and worst score at 0, where each class contributes to the score relative to its frequency in the dataset.",
        "paper_reference": "grandini2020metrics",
        "maximize": true,
        "image": "openproblems",
        "task_id": "label_projection",
        "commit_sha": "b3456fd73c04c28516f6df34c57e6e3e8b0dab32",
        "metric_id": "f1",
        "implementation_url": "https://github.com/openproblems-bio/openproblems/blob/main/openproblems/tasks/label_projection/metrics/f1.py"
    },
    {
        "metric_name": "Macro F1 score",
        "metric_summary": "The macro F1 score is an unweighted F1 score, where each class contributes equally, regardless of its frequency.",
        "paper_reference": "grandini2020metrics",
        "maximize": true,
        "image": "openproblems",
        "task_id": "label_projection",
        "commit_sha": "b3456fd73c04c28516f6df34c57e6e3e8b0dab32",
        "metric_id": "f1_macro",
        "implementation_url": "https://github.com/openproblems-bio/openproblems/blob/main/openproblems/tasks/label_projection/metrics/f1.py"
    }
]