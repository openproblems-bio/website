---
title: Batch integration feature
engine: knitr
image: thumbnail.png
---

```{r noninteractive}
#| include: FALSE
path <- '.'
task_id <- 'batch_integration_feature'
```




```{r setup}
#| include: FALSE
library(tidyverse)
library(reticulate)
library(funkyheatmap)
library(kableExtra)
op <- reticulate::import("openproblems")

# orig_settings <- knitr::opts_chunk$get()

task <- op$tasks[[task_id]]

method_info <- map_df(task$METHODS, function(fun) {
  bind_cols(
    tibble(
      task_id = task_id,
      method_id = fun$`__name__`
    ),
    do.call(tibble, fun$metadata)
  )
})

metric_info <- map_df(task$METRICS, function(fun) {
  bind_cols(
    tibble(
      task_id = task_id,
      metric_id = fun$`__name__`
    ),
    do.call(tibble, fun$metadata)
  )
})

dataset_info <- map_df(task$DATASETS, function(fun) {
  bind_cols(
    tibble(
      task_id = task_id,
      dataset_id = fun$`__name__`
    ),
    do.call(tibble, fun$metadata)
  )
})

parse_size <- function(x) {
  num <- as.numeric(gsub("[^\\.0-9]", "", x))
  denom <- gsub("[^A-Z]", "", x)
  mult <- c(MB = 1, GB = 1024, TB = 1024*1024)[denom]
  num * mult
}

json_paths <- list.files(path, pattern = "*.raw.json", full.names = TRUE)
results <- map_df(json_paths, function(json_path) {
  cat("Processing '", json_path, "'\n", sep = "")
  # replace NaNs and Infs
  json_txt <- readr::read_lines(json_path) %>%
    gsub(": NaN", ": \"NaN\"", .) %>%
    gsub(": Infinity", ": \"Infinity\"", .)
  data <- jsonlite::fromJSON(json_txt)

  dataset_id <- gsub(".*/", "", json_path) %>% gsub(".raw.json", "", .)
  
  process_scores <- function(x) {
    x[map_lgl(x, function(y) y == "NaN")] <- NA_real_
    x[map_lgl(x, function(y) y == "Infinity")] <- Inf
    do.call(tibble, x)
  }
  
  map2_df(names(data), data, function(method_id, li) {
    meta <- tibble(task_id, method_id, dataset_id)
    scaled <- process_scores(li$metrics)
    colnames(scaled) <- paste0("scaled_", colnames(scaled))
    raw <- process_scores(li$metrics_raw)
    exec <- do.call(tibble, li[!names(li) %in% c("metrics", "metrics_raw")]) %>%
      transmute(
        submit = lubridate::as_datetime(submit),
        duration = lubridate::duration(toupper(duration)),
        realtime = lubridate::duration(toupper(realtime)),
        cpu_pct = `%cpu` %>% gsub("%", "", .) %>% as.numeric(),
        peak_rss_mb = parse_size(peak_rss),
        peak_vmem_mb = parse_size(peak_vmem),
        rchar_mb = parse_size(rchar),
        wchar_mb = parse_size(wchar),
        code_version
      )
    bind_cols(meta, raw, scaled, exec)
  })
}) %>%
  left_join(method_info %>% select(method_id, is_baseline), "method_id")

qc <- tibble(
  task_id = character(0),
  section = character(0),
  name = character(0),
  desc = character(0),
  variable = character(0),
  value = numeric(0),
  lower = numeric(0),
  upper = numeric(0)
)
```

## Pre-process raw scores

Build a full crossing to make sure no results are missing.
It's likely some of the methods didn't finish running on all datasets.

```{r}
cross_df <- crossing(
  dataset_info %>% select(dataset_id),
  method_info %>% select(method_id, is_baseline),
  metric_info %>% select(metric_id)
)
```

Transform the results into a long format and join with the crossing.
```{r}
results_long <- 
  results %>%
  gather(metric_id, value, !!metric_info$metric_id) %>%
  select(method_id, dataset_id, metric_id, value, is_baseline) %>%
  full_join(cross_df, by = colnames(cross_df))
```

```{r}
#| include: false
# add some qc checks
qc <- qc %>% 
  bind_rows(
    tibble(
      task_id,
      section = "Raw data",
      name = "Long table size",
      desc = "Whether the long form of the results table has the right number of rows.",
      value = nrow(results_long),
      lower = nrow(method_info) * nrow(dataset_info) * nrow(metric_info),
      upper = nrow(method_info) * nrow(dataset_info) * nrow(metric_info)
    ),
    tibble(
      task_id,
      section = "Raw data",
      name = "Percentage of missing results",
      desc = "Probably shouldn't be higher than 10%.",
      value = mean(is.na(results_long$value)),
      lower = 0,
      upper = .1
    ),
    results_long %>%
      group_by(variable = metric_id) %>%
      summarise(
        task_id,
        section = "Raw data",
        name = paste0("Percentage of missing results per metric"),
        desc = "Probably shouldn't be higher than 10%.",
        value = mean(is.na(value)),
        lower = 0,
        upper = .1,
        .groups = "drop"
      ),
    results_long %>%
      group_by(variable = dataset_id) %>%
      summarise(
        task_id,
        section = "Raw data",
        name = paste0("Percentage of missing results per dataset"),
        desc = "Probably shouldn't be higher than 10%.",
        value = mean(is.na(value)),
        lower = 0,
        upper = .1,
        .groups = "drop"
      ),
    results_long %>%
      group_by(variable = method_id) %>%
      summarise(
        task_id,
        section = "Raw data",
        name = paste0("Percentage of missing results per method"),
        desc = "Probably shouldn't be higher than 10%.",
        value = mean(is.na(value)),
        lower = 0,
        upper = .1,
        .groups = "drop"
      )
  )
```


Plot the raw scores.

```{r include=FALSE}
knitr::opts_chunk$set(
  fig.width = 10,
  fig.height = nrow(method_info) * nrow(metric_info) / 4
)
```

```{r}
ggplot(results_long) +
  geom_point(aes(value, method_id, colour = is_baseline)) +
  facet_wrap(~metric_id, ncol = 1, scales = "free") +
  theme_bw() +
  labs(x = NULL, y = NULL)
```


## Compute scaling factors

* Compute the minimum and maximum scores of baseline methods per dataset per metric.
* Rescale values


```{r}
scaling_factors <- 
  results %>%
    filter(is_baseline) %>%
    gather(metric_id, value, !!metric_info$metric_id) %>%
    group_by(dataset_id, metric_id) %>%
    summarise(
      scale_min = ifelse(sum(!is.na(value)) == 0, 0, min(value, na.rm = TRUE)),
      scale_max = ifelse(sum(!is.na(value)) == 0, 1, max(value, na.rm = TRUE)),
      .groups = "drop"
    )
```

Visualise the scaling factors.

```{r include=FALSE}
knitr::opts_chunk$set(
  fig.width = 10,
  fig.height = nrow(metric_info) / 4
)
```

```{r, eval=FALSE, echo=FALSE}
scaling_factors %>% 
  gather(scaling_factor, value, scale_min, scale_max) %>%
  ggplot() +
  geom_point(aes(value, metric_id, colour = scaling_factor)) +
  theme_bw()
```



```{r}
results_long_scaled <- results_long %>%
  left_join(scaling_factors, by = c("dataset_id", "metric_id")) %>%
  left_join(metric_info %>% select(metric_id, maximize), by = "metric_id") %>%
  mutate(
    scaled_score = case_when(
      !is.na(value) ~ value,
      maximize ~ scale_min,
      !maximize ~ scale_max
    ),
    scaled_score = (scaled_score - scale_min) / (scale_max - scale_min),
    scaled_score = ifelse(maximize, scaled_score, 1 - scaled_score)
  )
```


```{r include=FALSE}
knitr::opts_chunk$set(
  fig.width = 4,
  fig.height = 4 * nrow(metric_info)
)
```

```{r, eval=FALSE, echo=FALSE}
ggplot(results_long_scaled) +
  geom_point(aes(value, scaled_score)) +
  facet_wrap(~metric_id, ncol = 1, scales = "free") +
  theme_bw()
```


```{r}
#| include: false
# add some qc checks
qc <- qc %>% 
  bind_rows(
    results_long_scaled %>%
      group_by(variable = metric_id) %>%
      summarise(
        task_id,
        section = "Metric scaling",
        name = paste0(c("Lower", "Upper"), " bound check scores after scaling"),
        desc = "Scores should not fall drastically outside the range of the baseline [min, max] range.",
        value = c(min(scaled_score), max(scaled_score)),
        lower = -1,
        upper = 2,
        .groups = "drop"
      )
  )
```


```{r}
overall_ranking <- results_long_scaled %>%
  group_by(method_id) %>%
  summarise(mean_score = mean(scaled_score)) %>%
  arrange(desc(mean_score))
```


View results

```{r include=FALSE}
knitr::opts_chunk$set(
  fig.width = 10,
  fig.height = nrow(method_info) * nrow(metric_info) / 4
)
```

```{r}
# order by ranking
results_long_scaled$method_id <- factor(results_long_scaled$method_id, levels = rev(overall_ranking$method_id))

ggplot(results_long_scaled %>% arrange(method_id)) +
  geom_vline(aes(xintercept = x), tibble(x = c(0, 1)), linetype = "dashed", alpha = .5, colour = "red") +
  geom_path(aes(scaled_score, method_id, group = dataset_id), alpha = .25) +
  geom_point(aes(scaled_score, method_id, colour = is_baseline)) +
  facet_wrap(~metric_id, ncol = 1, scales = "free") +
  theme_bw() +
  labs(x = NULL, y = NULL)
```


## Overview

Add extra columns
```{r}
per_dataset <- results_long_scaled %>%
  group_by(method_id, dataset_id) %>%
  summarise(score = mean(scaled_score), .groups = "drop") %>%
  mutate(dataset_id = paste0("dataset_", dataset_id)) %>%
  spread(dataset_id, score)
per_metric <- results_long_scaled %>%
  group_by(method_id, metric_id) %>%
  summarise(score = mean(scaled_score), .groups = "drop") %>%
  mutate(metric_id = paste0("metric_", metric_id)) %>%
  spread(metric_id, score)

summary <- 
  method_info %>%
  transmute(
    method_id,
    name_tmp = method_name,
    method_name = gsub(" \\(.*", "", name_tmp),
    method_config = gsub("[^\\(]*\\(?([^\\)]*)\\)?", "\\1", name_tmp),
    method_is_baseline = ifelse(is_baseline, "yes", "")
  ) %>%
  select(-name_tmp) %>%
  left_join(overall_ranking, by = "method_id") %>%
  left_join(per_dataset, by = "method_id") %>%
  left_join(per_metric, by = "method_id") %>%
  arrange(desc(mean_score))
```

```{r}

# fix funkyheatmap defaults so we don't need to do the processing below
column_info <- tibble(
  id = colnames(summary)[-1],
  name = id %>%
    gsub("^[^_]+_", "", .) %>%
    gsub("_", " ", .) %>%
    str_to_title(),
  group = gsub("_.*", "", id),
  geom = case_when(
    group == "method" ~ "text",
    group == "mean" ~ "bar",
    group %in% c("dataset", "metric") ~ "funkyrect"
  ),
  palette = ifelse(group %in% c("mean", "dataset", "metric"), group, NA_character_),
  options = map2(id, geom, function(id, geom) {
    if (id == "method_name") {
      list(width = 6, hjust = 0)
    } else if (id == "method_config") {
      list(width = 4)
    } else if (id == "is_baseline") {
      list(width = 1)
    } else if (geom == "bar") {
      list(width = 4)
    } else {
      list()
    }
  })
)

g <- funky_heatmap(
  data = summary,
  column_info = column_info,
  expand = c(xmax = 3),
  col_annot_offset = 4
)
```

```{r include=FALSE}
knitr::opts_chunk$set(
  fig.width = g$width,
  fig.height = g$height
)
```

```{r}
g
```

## Quality control

```{r}
#| echo: false
qc_plot <- qc %>% mutate(
  passes = lower <= value & value <= upper,
  lf = (value - lower) / ifelse(upper == lower, 1, upper - lower),
  test = case_when(
    lf <= 1 ~ "✓",
    lf <= 2 ~ "✗",
    lf <= 3 ~ "✗✗",
    TRUE ~ "✗✗✗"
  ),
  color = ifelse(lf <= 1, "green", "red")
) %>%
  arrange(desc(lf), section, name, variable)

qc_plot %>% 
  transmute(
    section,
    name,
    variable = ifelse(is.na(variable), "", variable),
    lower = round(lower, 3),
    value = round(value, 3),
    upper = round(upper, 3),
    test
  ) %>%
  kbl() %>%
  kable_paper() %>%
  column_spec(
    7, 
    color = qc_plot$color
  ) %>%
  column_spec(
    1:7,
    popover = qc_plot$desc
  )
```


## Raw data

:::{.column-page}

::: {.panel-tabset}

## Methods
```{ojs}
//| echo: false
Inputs.table(method_info)
```


## Metrics
```{ojs}
//| echo: false
Inputs.table(metric_info)
```

## Datasets
```{ojs}
//| echo: false
Inputs.table(dataset_info)
```

## Results
```{ojs}
//| echo: false
Inputs.table(results)
```

## Scaling factors
```{ojs}
//| echo: false
Inputs.table(scaling_factors)
```

## Summary
```{ojs}
//| echo: false
Inputs.table(summary)
```

## Quality control
```{ojs}
//| echo: false
Inputs.table(qc)
```

:::

:::


```{r pass-data-to-ojs}
#| echo: false
ojs_define(
  method_info_t = method_info,
  metric_info_t = metric_info,
  dataset_info_t = dataset_info,
  results_t = results,
  scaling_factors_t = scaling_factors,
  summary_t = summary,
  qc_t = qc
)
```

```{ojs transpose-ojs-to-r}
//| echo: false
method_info = transpose(method_info_t)
metric_info = transpose(metric_info_t)
dataset_info = transpose(dataset_info_t)
results = transpose(results_t)
scaling_factors = transpose(scaling_factors_t)
summary = transpose(summary_t)
qc = transpose(qc_t)
```
