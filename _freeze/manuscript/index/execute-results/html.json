{
  "hash": "425213c3313659740c82cb23568b8063",
  "result": {
    "markdown": "---\ntitle: Defining challenges and benchmarking open problems in Single-cell analysis\nengine: knitr\nbibliography: library.bib\n---\n\n\n\n\n\n## Abstract\n\n\nSingle-cell genomics has enabled the study of biological processes at an unprecedented scale and resolution [@celltypeatlas_plass2018; @humancellatlas_cao2020; @revisedairwayepithelial_montoro2018]. These studies have been enabled by computational tools for the analysis of single-cell data, which has surpassed 1300 published algorithms [@overtoolsreveal_zappia2021]. Evolving biological questions and technological development [@methodyearspatially_nmeth2021; @methodyearsinglecell_nmeth2020] continuously pose new challenges for data analysts, which has led to the description of grand challenges for single-cell data science [@elevengrandchallenges_lahnemann2020]. While the identification of such grand challenges in single-cell genomics can concentrate research into relevant areas, the current definitions of these challenges are qualitative in nature.\n\nClear definitions of open problems including quantifications of progress has driven innovation in machine learning research [@yearsdatascience_donoho2017]. Open challenges such as the Netflix prize [@netflixprize_bennet2007] and ImageNet [@largescalehierarchical_deng2009] represent landmark problems that allow measurement of progress across decades. These challenges have provided a common vocabulary for innovation: image classification accuracy has improved from 50% to over 90% on ImageNet since 2011. Inspired by open challenges in machine learning, Critical Assessment of protein Structure Prediction (CASP), Recursion’s cellular image classification Kaggle competition, various DREAM challenges, and our recent multimodal data integration competition at NeurIPS 2021 [@sandboxpredictionintegration_luecken2021] have focused research efforts in computational biology disciplines such as protein structure prediction, cellular diagnostics, gene module prediction, and multimodal single-cell data integration. Particularly successful competitions such as CASP have built communities that accelerate research toward these goals.\n\nIn single-cell genomics, large-scale benchmarks [@comparisonsinglecell_saelens2019; @benchmarkingatlaslevel_luecken2020; @biasrobustnessscalability_soneson2018; @confrontingfalsediscoveries_squair2021] have quantified open questions by proposing evaluation metrics and using these to compare existing methods. Yet, benchmarks inevitably age: newly developed tools that optimize the proposed benchmarking metrics cannot be compared in the same study; existing tools may perform differently on newly generated benchmarking datasets of better quality; and used metrics may not capture the aspects of method performance that a user is interested in. These challenges can only be addressed by community participation in benchmarking. To build such communities around quantified challenges, benchmarking studies would require frequent updating and open discussion on benchmarking datasets, proposed metrics, and selected methods.\n\nLearning from machine learning competitions, we propose a living benchmarking framework for single-cell genomics tools that follows the Common Task Framework (CTF) [@yearsdatascience_donoho2017]. The CTF has 3 criteria: (1) An easily available training dataset of sufficient complexity, (2) a set of competitors who submit methods for benchmarking, and (3) a set of metrics that are evaluated on test data to define progress on the task and thereby formally define it. When continuously updating such benchmarks, ...\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntasks <- map(op$TASKS, function(task) {\n  list(\n    method_ids = map_chr(task$METHODS, function(fun) fun$`__name__`),\n    metric_ids = map_chr(task$METRICS, function(fun) fun$`__name__`),\n    dataset_ids = map_chr(task$DATASETS, function(fun) fun$`__name__`)\n  )\n})\nn_tasks <- length(tasks)\nn_methods <- length(unique(unlist(map(tasks, \"method_ids\"))))\nn_metrics <- length(unique(unlist(map(tasks, \"metric_ids\"))))\nn_datasets <- length(unique(unlist(map(tasks, \"dataset_ids\"))))\n```\n:::\n\n\nWe have developed the Open Problems in Single-cell Analysis (Open Problems) framework, a platform that defines and measures progress towards open challenges in single-cell genomics following the CTF. Open Problems combines an open github repository with community-defined tasks, Github Actions testing workflows, a benchmarking pipeline using Nextflow [@nextflowenablesreproducible_tommaso2017] and Amazon Web Services, and a website to explore the results to build a living, community-based benchmarking project. Currently, Open Problems includes 11 defined tasks, on which 26 datasets are used to evaluate 119 methods using 31 metrics. These tasks were defined by interactions between contributors, which has led to new benchmarking insights even when contributors had already published benchmarks on these tasks. Open Problems provides community-defined standards for progress in single-cell data science to enable decentralized method development towards a common goal.\n\n\n\n## An infrastructure for living benchmarks in Single-cell Data Science\n\nIn order to enable a truly living benchmark, we have designed a standardized and automated infrastructure which allows members of the single-cell community to contribute to Open Problems in a seamless manner. Each Open Problems task is comprised of datasets, methods, and metrics (Fig 1): datasets define both the input and the ground truth for a task, methods attempt to solve the task, and metrics evaluate the success of a method on a given dataset. Metrics are normalized between 0 and 1 based on the inclusion of “baseline” methods which are designed to emulate either random or perfect performance. Methods are then ranked on a per-dataset basis by the average normalized metric score and presented in a summary table on the Open Problems website ([openproblems.bio](https://openproblems.bio)).\n\nTo enable seamless community involvement in Open Problems, we have designed the infrastructure to take advantage of automated workflows through GitHub Actions, Nextflow [@nextflowenablesreproducible_tommaso2017], and AWS Batch. When a community member adds a task, dataset, method, or metric, the new contributions are automatically tested on the cloud. When all tests pass and the new contribution is merged into the main repository, the results from the new contribution are automatically submitted to the Open Problems website. To maximize reproducibility, all code is run within Docker containers and all data is downloaded from public repositories, including figshare, GEO, and CELLxGENE [@cellxgeneperformantscalable_megill2021]. Task definitions, choices of metrics, and implementations of methods care discussed on our github repository ([github.com/openproblems-bio/openproblems](https://github.com/openproblems-bio/openproblems)) and can be easily amended by pull requests which are reviewed by task leaders (who initially defined the task) and the core infrastructure team.\n\n\n## Community-defined open problems in Single-cell analysis\n\n\nBuilding on previous work defining open challenges in single-cell analysis [@elevengrandchallenges_lahnemann2020] and many independent benchmarking studies in single-cell genomics [@benchmarkingatlaslevel_luecken2020; @biasrobustnessscalability_soneson2018; @benchmarkingspatialsingle_li2022; @systematicevaluationsingle_hou2020; @tuningparametersdimensionality_raimundo2020; @comprehensivecomparisonsupervised_sun2022; @accuracyrobustnessscalability_sun2019; @evaluationmachinelearning_huang2021; @flexiblecomparisonbatch_chazarragil2021; @benchmarkingjointmultiomics_cantini2021; @benchmarkingcelltype_avilacobos2020], we defined 11 Open Problems tasks (Fig 2a). While some tasks were directly transferred from published benchmarking papers (e.g., batch correction [@benchmarkingatlaslevel_luecken2020]), others were defined directly by method developers in the single-cell community (e.g., spatial decomposition). To exemplify how such a task is defined and the value that the task adds, we elaborate on the spatial deconvolution task.\n\n\n\n## Proposed structure\n\n\n* Task overview\n* Results from a few tasks\n  - All of the scIB benchmarking can be migrated into this framework (highly extensible)\n  - Addition of completely novel tasks (e.g. gene regulatory prediction)\n  - Community-driven benchmarking for emerging methodology (e.g. differential abundance, spatial decomposition)\n  - Results of the CZI jamboree\n* Focus on spatial decomposition task\n  - Explain metrics, datasets, methods\n\n\n## Defined open problems facilitate/drive innovation in single-cell data science\n\n* Reaching out to the ML community:\n  - NeurIPS competition built on Open Problems\n    * Note: Community-informed re-evaluation of metrics (PMLR report)\n* Using framework to drive development of novel methods\n* Enable method developers to include their methods in live benchmark and publish results\n* Build analysis pipelines from top performers across tasks\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}