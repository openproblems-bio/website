---
title: 'Neurips 2021'
$toc:
  - text: Neurips 2021 Documentation
    children:
    - text: Using the documentation
    - text: About
      children:
      - text: Dates
      - text: Discord
      - text: Explore
        children:
        - text: Getting started
        - text: Accessing the free upgrade for competitors
        - text: Getting help with Saturn Cloud
      - text: Get started
    - text: Tasks
      children:
      - text: Modality prediction
        children:
        - text: Predicting the flow of information from DNA to RNA and RNA to Protein
        - text: Task API
          to: '#tapi-1'
          children:
          - text: Input data formats
            to: '#idf-1'
            children:
            - text: Inputs to methods
              to: '#itm-1'
            - text: Objective for methods
              to: '#ofm-1'
            - text: Attributes of input data
              to: '#aoid-1'
            - text: Normalization and transformation of data for the prediction task
          - text: Output data formats
            to: '#odf-1'
          - text: Metric
            to: '#met-1'
        - text: Prizes
          to: '#prizes-1'
      - text: Modality matching
        children:
        - text: Matching profiles of each cell from different modalities
        - text: Task API
          to: '#tapi-2'
          children:
          - text: Input data formats
            to: '#idf-2'
            children:
            - text: Inputs to methods
              to: '#itm-2'
            - text: Objective for methods
              to: '#ofm-2'
            - text: Attributes of input data
              to: '#aoid-2'
            - text: Normalization and transformation of data for the matching task
          - text: Output data formats
            to: '#odf-2'
          - text: Metric
            to: '#met-2'
        - text: Prizes
          to: '#prizes-2'
      - text: Joint Embedding
        children:
        - text: Learn a joint embedding for multiple modalities 
        - text: Task API
          to: '#tapi-3'
          children:
          - text: Input data formats
            to: '#idf-3'
            children:
            - text: Normalization and transformation of data for the joint embedding task
          - text: Output data formats
            to: '#odf-3'
          - text: Metrics
            to: '#met-3'
            children:
            - text: Batch removal metrics
            - text: Metric aggregation
          - text: Prizes
            to: '#prizes-3'
      - text: Data
        children:
        - text: The flow of genetic information in cells
        - text: A rough history of single-cell technologies
        - text: Measuring multiple modalities in single cells
        - text: Multimodal data as a basis for benchmarking
      - text: Benchmarking
        children:
        - text: Download the data
        - text: Data file format
        - text: Preprocessing
          children:
          - text: Preprocessing of gene expression (GEX)
          - text: Preprocessing of ATAC
          - text: Preprocessing of protein abundance (ADT)
        - text: Metadata
          children:
          - text: Gene expression observation metadata
          - text: Gene expression feature metadata
          - text: ATAC observation metadata
          - text: ATAC Feature metadata
      - text: Submission
        children:
        - text: Development Process
          children:
          - text: Editing the script and config file
          - text: Manually running the component
          - text: Unit test the component
          - text: Evaluate locally
          - text: Advanced topics
            children:
            - text: Use additional files
          - text: Troubleshooting
    - text: FAQ
      children:
      - text: What if I only want to compete for one of the prizes in a task?
      - text: Can I store helper functions as separate files?
        children:
        - text: Python
        - text: R
      - text: How can I upload a pre-trained model?
      - text: Can I pre-train on public data?
      - text: How are the libraries in `config.vsh.yaml` installed?
      - text: Error message "Process terminated with an error exit status (xxx)"
        children:
        - text: Interpret exit codes
        - text: Solving exit code 1
        - text: Solving exit code 137
      - text: How can I increase the memory/CPU/runtime limits of my method?
      - text: My submission is stuck at status 'Submitted'
      - text: How does the Nextflow execution in `2_generate_submission.sh` work?
      - text: Can I use an Nvidia GPU to train the model
        children:
        - text: Set up local system
        - text: Update starter kit
      - text: How to generate a submission from WSL2
      - text: Can I generate a submission on Saturn Cloud?
    - text: Quickstart SaturnCloud
      children:
      - text: 1. Register on EvalAI
      - text: 2. Register on Saturn Cloud
      - text: 3. Confgiure your local environment
      - text: 4. Grab a starter kit
      - text: 5. Tweak starter kit
      - text: 6. Generate your first submission
      - text: 7. Submitting to EvalAI
      - text: Encountering issues?
    - text: Quickstart EvalAI
      children:
      - text: 1. Register on EvalAI
      - text: 2. Configure your local environment
      - text: 3. Grab a starter kit
      - text: 4. Generate your first submission
      - text: 5. Submitting to EvalAI
      - text: Encountering issues?
    - text: Start Kit Contents
      children:
      - text: Anatomy of `script.py`
        children:
        - text: Imports
        - text: Method
      - text: Anatomy of `config.vsh.yaml`
        children:
        - text: Functionality
        - text: Platform
      - text: Input/output AnnData file format
    - text: Terms
      children:
      - text: Terms and Conditions
      - text: Eligibility
      - text: Participation
      - text: Submission
      - text: Determining Winners
      - text: Winner's Obligations
      - text: Prizes
      - text: Publicity
      - text: Privacy
      - text: Warranty
      - text: Indemnity
      - text: Right to Cancel, Modify, or Disquality
      - text: No Offer of Employment
      - text: Severability
      - text: Waiver
      - text: Integration
    - text: 2021 Competition
      children:  
      - text: Multimodal Single-Cell Data Integration
      - text: Details
      - text: Organizers
        children:
        - text: Organizing Team
      - text: Sponsors
      - text: Summary
      - text: Winners
        children: 
        - text: Task 1 - Modality Prediction
          children:
          - text: GEX→ATAC - Living Systems Lab
          - text: ATAC→GEX - Cajal
          - text: GEX→ADT - Dengkw
          - text: ADT→GEX - Novel
          - text: Overall - DANCE
        - text: Task 2 - Match Modality
          children:
          - text: Winner in all categories - CLUE
        - text: Task 3 - Joint Embedding
          children:
          - text: Multiome, pre-trained & CITE, pre-trained - Amateur
          - text: Multiome, online - Living Systems Lab
          - text: CITE, online - Dengkw
---

**Important update: The validation phase has been shifted by one week. Please check the [documentation](/neurips_docs/about/dates/) for the update dates.**

<m-d-header text="Using the documentation" h=2 add-hash></m-d-header>
Welcome to the documentation site for the multimodal single-cell data integration 2021 NeurIPS competition! This site is home to all the documentation you need to get started competing in the competition. On the table of contents to the left, you'll see information about the competition, the tasks, and the submission process.

If you ever have any questions, please feel free to reach out on [the Open Problems Discord Server](https://discord.gg/hDE5bYEcHF).

<m-d-header text="About" h=1 add-hash></m-d-header>

This is an official challenge in the [NeurIPS 2021 Competition Track](https://neurips.cc/Conferences/2021/CompetitionTrack). The competition is hosted by Open Problems in Single-Cell Analysis with sponsorship from [CZI](https://czi.org) and [Cellarity](https://cellarity.com). We are hosting the competition on [EvalAI](https://eval.ai/web/challenges/challenge-page/1111/overview), which is where competitors will submit code and can view a public leaderboard.

The competitions is broken down into three tracks, each with an independent set of prizes. Details about each task can be found in the [Tasks Documentation](/neurips_docs/about_tasks).

The competition will run in two phases, dates can be found [here]():

* **Development (Sept 15 - Nov 15)** - during this development phase, participants will prototype submissions for the competition on the training data. Users will have access to starter kits with baseline methods and scripts to download code. Users can submit code to EvalAI for ranking on a public leaderboard using a subset of the public training data. These rankings are meant to help drive competition during the competition but will not be used to award prizes.
* **Testing (Nov 15 - Nov 26)** - during the testing phase, competitors will submit their final code packages for evaluation on held-out testing data. Submissions will run on our EvalAI worker, and must be formatted according to the format in the associated starter kit for each task.

Winners will receive a cash price according to the prize criteria described in each task description. [Terms and conditions](/neurips_docs/submission/terms) apply.

For more information about how to get started, visit our [Getting Started](/neurips_docs/about/getting_started) instructions.

<m-d-header text="Dates" h=2 add-hash></m-d-header>


The following dates describe the different milestones for the competition.

**Sep 15** - Competition Kickoff: Release starter kits and first batch of training & validation data  
**Sep 22 - Nov 15** - Phase 1: Open Leaderboard and release second batch of training & validation data  
**Nov 15 - Nov 26** - Phase 2: Competitors submit code for evaluation on test data  
**Nov 30** - Notify winners  
**Dec 10** - NeurIPS Competition and Benchmarks Mini Workshop  

<m-d-header text="Discord" h=2 add-hash></m-d-header>


If you ever have any questions, please feel free to reach out on the [Open Problems Discord Server](https://discord.gg/hDE5bYEcHF).

You can find channels for the competition in the category "NeurIPS 2021 Competition".

<m-d-header text="Explore" h=2 add-hash></m-d-header>

To make this competition more accessible, we teamed up with [Saturn Cloud](https://saturncloud.io) to provide up to 100 hours of free compute per month to competition participants. Saturn Cloud hosts Jupyter servers that run behind the scenes on AWS. The Saturn Cloud website provides a GUI that allows you to launch cloud resources easily in your web browser.  We've created a custom image preloaded with the benchmarking dataset, several single-cell analysis tools, and exploratory notebooks.

<info-box>
  Currently, the Starter Kits require Docker. Because of the way Saturn Cloud creates Jupyter Servers, Docker isn't available within the Saturn Cloud instances.<br>
  As a result, you currently cannot run the <code>1_unit_test.sh</code>, <code>2_generate_submission.sh</code>, or <code>3_evaluate_submission.sh</code> scripts on Saturn Cloud.<br>
  We are working hard to get these scripts working on Saturn Cloud, but for now, you'll need to evaluate your methods on a resource with Docker installed.
</info-box>

<m-d-header text="Getting started" h=3 add-hash></m-d-header>

1. Go to https://saturncloud.io/ and login or create a free account
2. Under "Create a Resource" at the top of the page, click on the NeurIPS Openbio template card and click through the pop-up modals to create a new Saturn Cloud project using this template.
3. Click on the ▶ button to start a Jupyter server
4. Once the server is started, click the "Jupyter Lab" button to access the server
5. Open the data exploration notebooks in `~/project/explore/`

<m-d-header text="Accessing the free upgrade for competitors" h=3 add-hash></m-d-header>

From the Saturn Cloud dashboard, you should see a blue circle in the lower right corner with a which chat icon inside. Click this button and send a message saying "Can you please upgrade my account for the Open Problems NeurIPS competition?"

Someone at Saturn Cloud should respond shortly and you should see 100 hours appear under the "Hours remaining" box on the left sidebar.

<m-d-header text="Getting help with Saturn Cloud" h=3 add-hash></m-d-header>

Saturn Cloud has great documentation: https://saturncloud.io/docs/

You can also use the chat box in the lower right corner of the Dashboard to ask for help. Saturn Cloud can help you install packages, configure your environment, and more!


<m-d-header text="Get started" h=2 add-hash></m-d-header>
To get started:

1.  Read [about the competition](/neurips_docs/about/about),  [about the tasks](/neurips_docs/about_tasks) and the [submission quickstart](/neurips_docs/submission/quickstart/) on our competition website
2.  View the [starter kit contents](/neurips_docs/submission/starter_kit_contents)
3.  Explore the data and prototype methods for free on [Saturn Cloud](https://openproblems.bio/neurips_docs/about/explore) (Optional)
4.  Implement your method and [generate a submission](/neurips_docs/submission/development_process/)!


If you ever have any questions, please feel free to reach out on the [Open Problems Discord Server](https://discord.gg/hDE5bYEcHF).

You can find channels for the competition with the prefix #neurips2021

<m-d-header text="Tasks" h=1 add-hash></m-d-header>

The competition will focus on three tasks. Click on each header to learn more.
<m-d-header text="Modality prediction" h=2 add-hash></m-d-header>

<m-d-header text="Predicting the flow of information from DNA to RNA and RNA to Protein" h=3 add-hash></m-d-header>

Experimental techniques to measure multiple modalities within the same single cell are increasingly becoming available. The demand for these measurements is driven by the promise to provide a deeper insight into the state of a cell. Yet, the modalities are also intrinsically linked. We know that DNA must be accessible (ATAC data) to produce mRNA (expression data), and mRNA in turn is used as a template to produce protein (protein abundance). These processes are regulated often by the same molecules that they produce: for example, a protein may bind DNA to prevent the production of more mRNA. Understanding these regulatory processes would be transformative for synthetic biology and drug target discovery. Any method that can predict a modality from another must have accounted for these regulatory processes, but the demand for multi-modal data shows that this is not trivial.

<figure>
  <img src="/media/tasks/predict.svg">
  <figcaption>
    <h3>
      Task 1: Prediction
    </h3>
    <p style="font-size: medium;">
      In this task, the goal is to take one modality (ATAC or RNA) and predict the other modality (RNA or Protein) for all features in each cell (only ATAC + RNA shown). Performance is measured using Root Mean Square Error on log1p size-factor normalized counts.
    </p>
  </figcaption>
</figure>

This task requires translating information between multiple layers of gene regulation. In some ways, this is similar to the task of machine translation. In machine translation, the same sentiment is expressed in multiple languages and the goal is to train a model to represent the same meaning in a different language. In this context, the same cellular state is measured in two different feature sets and the goal of this task is to translate the information about cellular state from one modality to the other. 

<m-d-header text="Task API" h=3 add-hash alternate="tapi-1"></m-d-header>

The following section describes the task API for the Modality Prediction task. Competitors must submit their code as a Viash component. To facilitate creation of these components, [starter kits](//neurips_docs/submission/starter_kit_contents) have been provided.

<m-d-header text="Input data formats" h=4 add-hash alternate="idf-1"></m-d-header>

<info-box>
  The data format and attributes provided in the input data is tailored to each task and may differ from the publicly released <a href='neurips/2021#benchmarking'>benchmarking dataset</a>.  <b>Only the attributes listed in the following section will be accessible to methods submitted to the competition.</b>
</info-box>

<m-d-header text="Inputs to methods" h=5 add-hash alternate="itm-1"></m-d-header>

Method components should expect three inputs, `--input_train_mod1`, `--input_train_mod2`, and `--input_test_mod1`. They are all paths to [AnnData](https://anndata.readthedocs.io/en/latest/) h5ad files with the attributes below. More information can be found on AnnData objects [here](/neurips_docs/submission/quickstart/). `mod1` and `mod2` refer to the modality of the datasets as defined by `feature_type`.  One file will always have `feature_type` be `"GEX"` and the other will be `"ATAC"` or `"ADT"`. For the purposes of the competition, components should expect the following 4 combinations of modalities:

| `mod1`   | `mod2`   |
|----------|----------|
| `"GEX"`  | `"ATAC"` |
| `"ATAC"` | `"GEX"`  |
| `"GEX"`  | `"ADT"`  |
| `"ADT"`  | `"GEX"`  |


<m-d-header text="Objective for methods" h=5 add-hash alternate="ofm-1"></m-d-header>

Submission components must predict `mod2` for the cells provided in `--input_test_mod1`. For methods that do not involve pre-trained models, training data is also provided in the `--input_train_mod[1|2]` files. For methods that involve pre-trained models, these training datasets can be ignored.

<info-box>
  Note, you do not need to return predictions for all four combinations of inputs and outputs. We will be independently ranking and awarding prizes to each combination as described below in <a href="#prizes">Prizes</a>. For more details, see the <a href="/neurips/2021#what-if-i-only-want-to-compete-for-one-of-the-prizes-in-a-task">FAQs</a>.
</info-box>

<m-d-header text="Attributes of input data" h=5 add-hash alternate="aoid-1"></m-d-header>

The input data objects have the following attributes:


```plaintext
adata
  Input AnnData object for modality 1 or 2

  Attributes
  ----------
  adata.X : ndarray, shape=(n_obs, n_var)
    Sparse profile matrix of given modality. If .var['feature_types'] == "GEX" or "ADT",
    values in adata.X represent expression counts for each gene. If
    .var['feature_types'] == "ATAC", values represent counts of reads in peaks for
    chromatin accessibility
  adata.uns['dataset_id'] : str
    The name of the dataset.
  adata.obs["batch"] : ndarray, shape=(n_obs,)
    The batch from which the data was sequenced. Has format "s[1-4]d[1-9]" indicating the site and
    donor associated with the batch.
  adata.obs_names : ndarray, shape=(n_obs,)
    Ids for the cells.
  adata.var['feature_types']: ndarray, shape=(n_var,)
    The modality of this file, should be equal to "GEX", "ATAC" or "ADT".
  adata.var_names : ndarray, shape=(n_var,)
    Ids for the features.
```

Examples of how to load and process the data are contained in the [starter kits](/neurips_docs/submission/starter_kit_contents) for the respective programming language.

<m-d-header text="Normalization and transformation of data for the prediction task" h=5 add-hash></m-d-header>

To make the task more straightforward, we have followed common practices for normalizing and transforming data of each modality. The raw data is also provided in `adata.layers` as described below. Please note, **the performance metric will be calculated on the normalized and transformed data stored in** `adata.X` **for each of the modality types below.**

For full details on preprocessing, see the [Data Preprocessing](/neurips_docs/data/dataset/#preprocessing) notes.

**GEX**  

For this task, gene expression data stored in `adata.X` for the training and test data has been size-factor normalized and log1p transformed.  Raw UMI counts are available in `adata.layers["counts"]`. Size factors are accessible in `adata.obs["size_factors"]`

**ATAC**

For this task, ATAC data stored in `adata.X` for the training and test data has been binarized and subset to 10000 random peaks. The raw UMI counts for each peak can be found in `adata.layers["counts"]`.

**ADT**

For this task, ADT derived protein abundance measures have been centered log-ration (CLR) normalized. Raw ADT counts can be found in `adata.layers["counts"]`.

<m-d-header text="Output data formats" h=5 add-hash alternate="odf-1"></m-d-header>

This component should output only one h5ad file whose path is specified via `--output`, containing the predicted profile values of modality 2 for the **test cells only**. It must have the following attributes:

```plaintext
adata
  Output AnnData object containing predictions for modality 2 in the "test" cells

  Attributes
  ----------
  adata.X : ndarray, shape=(n_obs, n_var)
    Sparse profile matrix.
  adata.uns['dataset_id'] : str
    The name of the dataset.
  adata.uns['method_id'] : str
    The name of the prediction method. This is used to track submissions.
  adata.var['feature_types'] : ndarray, shape=(n_var,)
    The modality of this file, should be equal to "GEX", "ATAC" or "ADT".
  adata.obs_names : ndarray, shape=(n_obs,)
    Ids for the cells.
```

<m-d-header text="Metric" h=4 add-hash alternate="met-1"></m-d-header>

Performance in task 1 is measured using the root mean squared error between the observed and predicted values for modality 2 in the `test` set. Lower values are better.

The metric function used to evaluate the prediction has the following structure (this example employs `Python` syntax; the `R` evaluation function is functionally equivalent):

```python
def calculate_rmse(adata_mod2, adata_mod2_answer):  
    '''Function to calculate MSE between prediction and solution for the test sets  

    Params
    ------
    adata_mod2 : AnnData, shape=(n_obs, n_var)
      User-submitted prediction for expression of mod2 in cells from the test set
    adata_mod2_answer : AnnData, shape=(n_obs, n_var)
      Measured values for expression of mod2 in the test set

    Returns
    -------
    mean_square_error : float
      The mean squared error between the predicted and observed values for all features in
      the test set.
    '''
    from sklearn.metrics import mean_square_error
    return mean_square_error(adata_mod2.X, adata_mod2_answer.X, squared=False)
```

<m-d-header text="Prizes" h=3 add-hash alternate="prizes-1"></m-d-header>

For this task, five prizes of &dollar;1000 will be awarded to the submissions for each of the following criteria:
1. Best performance predicting GEX → ATAC
2. Best performance predicting ATAC → GEX
2. Best performance predicting GEX → ADT
2. Best performance predicting ADT → GEX
3. Best performance on average across modalities

[Terms and Conditions](/neurips_docs/submission/terms/) apply.

<m-d-header text="Modality matching" h=2 add-hash></m-d-header>
<m-d-header text="Matching profiles of each cell from different modalities" h=3 add-hash></m-d-header>

While joint profiling of two modalities in the same single cell is now possible, most single-cell datasets that exist measure only a *single* modality. These modalities complement each other in their description of cellular state. Yet, it is challenging to analyse uni-modal datasets together when they do not share observations (cells) or a common feature space (genes, proteins, or open chromatin peaks). If we could map observations to one another across modalities, it would be possible to treat separately profiled datasets in the same manner as new multi-modal sequencing data. Mapping these modalities to one another opens up the vast amount of uni-modal single-cell datasets generated in the past years to multi-modal data analysis methods.

Unlike in task 1, where the goal was to predict all values of RNA or protein abundances from ATAC or RNA (respectively) in each cell, the goal of this task is to identify the correspondence between single-cell profiles. Because we are only interested in matching observations, the competitors are encouraged to consider feature selection to identify the representation of the input most important for matching observations.

<figure>
  <img src="/media/tasks/match.svg">
  <figcaption>
    <h3>
      Task 2: Matching
    </h3>
    <p style="font-size: medium;">
      In this task, competitors are given a jointly measured multimodal dataset (only ATAC + RNA shown) where the cell barcodes linking the two measures are obscured for the purposes of the competition. Competitors must estimate which profiles match across modalities and provide the probability distribution of these predictions. The sum of weights in the correct correspondences is used to score submissions.
    </p>
  </figcaption>
</figure>

<m-d-header text="Task API" h=3 add-hash alternate="tapi-2"></m-d-header>

The following section describes the task API for the Modality Matching task. Competitors must submit their code as a Viash component. To facilitate creation of these components, [starter kits](//neurips_docs/submission/starter_kit_contents) have been provided.

<m-d-header text="Input data formats" h=4 add-hash alternate="idf-2"></m-d-header>

<info-box>
  The data format and attributes provided in the input data is tailored to each task and may differ from the publicly released <a href="/neurips/2021#benchmarking">benchmarking dataset</a>. <b>Only the attributes listed in the following section will be accessible to methods submitted to the competition.</b>
</info-box>



<m-d-header text="Inputs to methods" h=5 add-hash alternate="itm-2"></m-d-header>

Method components should expect **five** h5ad files, `--input_train_mod1`, `--input_train_mod2`, `--input_train_sol`, `--input_test_mod1`, and `--input_test_mod2`.

The `*_mod1` and `*_mod2` arguments are paths to [AnnData](https://anndata.readthedocs.io/en/latest/) objects containing the modality 1 and modality 2 data for which the rows are shuffled and anonymized. These files have the attributes below. If the `feature_types` of one modality is ``"GEX"``, then that of the other must be either ``"ATAC"`` or ``"ADT"`` (antibody-derived tag, a measure of protein abundance). For the purposes of the competition, components should expect the following 4 combinations of modalities:

| `mod1`   | `mod2`   |
|----------|----------|
| `"GEX"`  | `"ATAC"` |
| `"ATAC"` | `"GEX"`  |
| `"GEX"`  | `"ADT"`  |
| `"ADT"`  | `"GEX"`  |

The `input_train_sol` file contains the solution pairing matrix of shape `(n_obs, n_obs)` indicating the true correpsondences from the joint profiling experiment in the input training data. A value of `1` in index `[i,j]` means that `input_train_mod1.X[i]` and `input_train_mod2.X[j]` were measured in the same cell. All other values are `0`.

  * `X`: The sparse pairing matrix. A value of 1 in this matrix means this modality 1 profile (row) corresponds to a modality 2 profile (column).
  * `.uns['dataset_id']`: The name of the dataset.

<m-d-header text="Objective for methods" h=5 add-hash alternate="ofm-2"></m-d-header>

The goal is to output a matching matrix that predicts the correspondences between profiles from modality 1 in modality 2.

<info-box>
  Note, you do not need to return predictions for all four combinations of inputs and outputs. We will be independently ranking and awarding prizes to each combination as described below in <a href="#prizes">Prizes</a>. For more details, see the [FAQs](/neurips_docs/submission/faq/#what-if-i-only-want-to-compete-for-one-of-the-prizes-in-a-task)
</info-box>

<m-d-header text="Attributes of input data" h=5 add-hash alternate="aoid-2"></m-d-header>

The input `*_mod1` and `*_mod2` data objects have the following attributes:


```plaintext
adata
  Input AnnData object for task 2

  Attributes
  ----------
  adata.X : ndarray, shape=(n_obs, n_var)
    Sparse profile matrix of given modality. If .var['feature_types'] == "GEX" or "ADT",
    values in adata.X represent expression counts for each gene. If
    .var['feature_types'] == "ATAC", values represent counts of reads in peaks for
    chromatin accessibility
  adata.obs['batch']: ndarray, shape=(n_obs,)
    The batch from which the data was sequenced. Has format "s[1-4]d[1-9]" indicating the site and
    donor associated with the batch.
  adata.uns['dataset_id'] : str
    The name of the dataset.
  adata.var['feature_types']: ndarray, shape=(n_var,)
    The modality of this file, should be equal to "GEX", "ATAC" or "ADT".
  adata.var_names : ndarray, shape=(n_var,)
    Ids for the features.
  adata.obs_names : ndarray, shape=(n_obs,)
    Anonymised ids for the cells.
```

The input `input_train_sol` object has the following attributes:

```plaintext
adata
  Input solution AnnData object for task 2

  Attributes
  ----------
  adata.X : ndarray, shape=(n_obs)
    Sparse pairing matrix for input_train_mod[1|2]
  adata.uns['dataset_id'] : str
    The name of the dataset.
  adata.uns['method_id']: str
    The name of the prediction method.
```

Examples of how to load and process the data are contained in the [starter kits](/neurips_docs/submission/starter_kit_contents) for the respective programming language.


<m-d-header text="Normalization and transformation of data for the matching task" h=5 add-hash></m-d-header>

To make the task more straightforward, we have followed common practices for normalizing and transforming data of each modality. The raw data is also provided in `adata.layers` as described below.

For full details on preprocessing, see the [Data Preprocessing](/neurips_docs/data/dataset/#preprocessing) notes.

**GEX**  

For this task, gene expression data stored in `adata.X` for the training and test data has been size-factor normalized and log1p transformed.  Raw UMI counts are available in `adata.layers["counts"]`. Size factors are accessible in `adata.obs["size_factors"]`

**ATAC**

For this task, ATAC data stored in `adata.X` for the training and test data has been binarized. The raw UMI counts for each peak can be found in `adata.layers["counts"]`.

**ADT**

For this task, ADT derived protein abundance measures have been centered log-ration (CLR) normalized. Raw ADT counts can be found in `adata.layers["counts"]`.

<m-d-header text="Output data formats" h=4 add-hash alternate="odf-2"></m-d-header>

This component should output only *one* h5ad file, `--output`, containing the predicted probabilities of the matchings between the two input datasets.

```plaintext
adata
  Output AnnData object for task 2

  Attributes
  ----------
  adata.X : ndarray, shape=(n_obs)
    Sparse pairing matrix.
  adata.uns['dataset_id'] : str
    The name of the dataset.
  adata.uns['method_id']: str
    The name of the prediction method.
```

<info-box>
  **Restrictions on the pairing matrix**

  There are two restrictions on the pairing matrix:
  1. Rows sum to 1 - The sum of each row of the matrix should be 1. This limits the amount of weight that can be placed on each pairing `i,j`
  2. Sparsity - If `input_mod1` has shape `(n_obs, n_var1)` and `input_mod2` has shape `(n_obs, n_var2)`, the pairing matrix must be a sparse matrix with shape `(n_obs, n_obs)` containing **at most `1000×N` non-zero values**. Predictions with more than `1000×N` non-zero values will be rejected by the metric component.
</info-box>


<m-d-header text="Metric" h=4 add-hash alternate="met-2"></m-d-header>

Performance in task 2 is measured using a weighted sum of the confidences placed on correct pairings `i,j`.
Specifically the score is calculated using the following equation:

<v-math :inline="false">Score = \frac{1}{N} \sum_i \sum_j X_{i,j} * \delta_{i,j}</v-math>

where <v-math>X_{i,j}</v-math> is the entry in the sparse pairing matrix from the user's submission and <v-math>\delta_{i,j}</v-math> is <v-math>1</v-math> if profile <v-math>i</v-math> and <v-math>j</v-math> were measured in the same cell and <v-math>0</v-math> otherwise. <v-math>N</v-math> is the number of observations.


<m-d-header text="Prizes" h=3 add-hash alternate="prizes-2"></m-d-header>

For this task, five &dollar;1000 prizes will be awarded to the submissions for each of the following criteria:
1. Best performance matching GEX → ATAC
2. Best performance matching ATAC → GEX
2. Best performance matching GEX → ADT
2. Best performance matching ADT → GEX
3. Best performance on average across modalities


<m-d-header text="Joint Embedding" h=2 add-hash></m-d-header>
<m-d-header text="Learn a joint embedding from multiple modalities" h=3 add-hash></m-d-header>

The functioning of organs, tissues, and whole organisms is determined by the interplay of cells. Cells are characterised into broad types, which in turn can take on different states. Here, a cell state is made up of the sum of all processes that are occurring within the cell. We can gain insight into the state of a cell by different types of measurements: e.g., RNA expression, protein abundance, or chromatin conformation. Combining this information to describe cellular heterogeneity requires the formation of joint embeddings generated from this multimodal data. These embeddings must account for and remove possible batch effects between different measurement batches. The reward for methods that can achieve this is great: a highly resolved description of the underlying biological state of a cell that determines its function, how it interacts with other cells, and thus the cell’s role in the functioning of the whole tissue.

<figure>
  <img src="/media/tasks/integrate.svg">
  <figcaption>
    <h3>
      Task 3: Joint embedding
    </h3>
    <p style="font-size: medium;">
      In this task, the goal is to learn an embedded space that leverages the information of multiple modalities (only ATAC + RNA shown). Quality of the embedding will be scored using a number of metrics derived from expert annotation.
    </p>
  </figcaption>
</figure>


<m-d-header text="Task API" h=3 add-hash alternate="tapi-3"></m-d-header>

The following section describes the task API for the Joint Embedding task. Competitors must submit their code as a Viash component. To facilitate creation of these components, [starter kits](//neurips_docs/submission/starter_kit_contents) have been provided.

<m-d-header text="Input data formats" h=3 add-hash alternate="idf-3"></m-d-header>

This component expects two inputs, `--input_mod1` and `--input_mod2`. They are both [AnnData](https://anndata.readthedocs.io/en/latest/), containing the full profile matrices where extra metadata has been removed. These have the following attributes:

```plaintext
adata
  Input AnnData object for task 2

  Attributes
  ----------
  adata.X : ndarray, shape=(n_obs, n_var)
    Sparse profile matrix of given modality. If .var['feature_types'] == "GEX" or "ADT",
    values in adata.X represent expression counts for each gene. If
    .var['feature_types'] == "ATAC", values represent counts of reads in peaks for
    chromatin accessibility
  adata.uns['dataset_id'] : str
    The name of the dataset.
  adata.var['feature_types']: ndarray, shape=(n_var,)
    The modality of this file, should be equal to "GEX", "ATAC" or "ADT".
  adata.var_names : ndarray, shape=(n_var,)
    Ids for the features.
  adata.obs_names : ndarray, shape=(n_obs,)
    Ids for the cells.
  adata.obs['batch']`: ndarray, shape=(n_obs,)
    The batch from which the data was sequenced. Has format "s[1-4]d[1-9]" indicating the site and
    donor associated with the batch.
  adata.obs['size_factors']`: ndarray, shape=(n_obs,)
    The batch from which the data was sequenced. Has format "s[1-4]d[1-9]" indicating the site and
    donor associated with the batch.
```


<m-d-header text="Normalization and transformation of data for the joint embedding task" h=5 add-hash></m-d-header>


To make the task more straightforward, we have followed common practices for normalizing and transforming data of each modality. The raw data is also provided in `adata.layers` as described below.

For full details on preprocessing, see the [Data Preprocessing](/neurips_docs/data/dataset/#preprocessing) notes.

**GEX**  

For this task, gene expression data stored in `adata.X` for the training and test data has been size-factor normalized and log1p transformed.  Raw UMI counts are available in `adata.layers["counts"]`. Size factors are accessible in `adata.obs["size_factors"]`

**ATAC**

For this task, ATAC data stored in `adata.X` for the training and test data has been binarized. The raw UMI counts for each peak can be found in `adata.layers["counts"]`.

**ADT**

For this task, ADT derived protein abundance measures have been centered log-ration (CLR) normalized. Raw ADT counts can be found in `adata.layers["counts"]`.

<m-d-header text="Output data formats" h=4 add-hash alternate="odf-3"></m-d-header>

This component should output only *one* h5ad file, `--output`, containing the predicted pairings of the two input datasets. We are limiting the embedding to at most 100 dimensions. The output file must have the following attributes.

```plaintext
adata
  Output AnnData object for task 2

  Attributes
  ----------
  adata.X : ndarray, shape=(n_obs, N), N <= 100
    Embedding matrix.
  adata.uns['dataset_id'] : str
    The name of the dataset.
  adata.uns['method_id']: str
    The name of the prediction method.
  adata.obs_names : ndarray, shape=(n_obs,)
    Ids for the cells.

```

<m-d-header text="Metrics" h=4 add-hash alternate="met-3"></m-d-header>

Performance in task 3 will be measured using 6 different metrics broken into two classes:
* Biology conservation
* Batch removal

These measures are then aggregated into a single score used to rank methods.

<m-d-header text="Nio-conservation metrics" h=5 add-hash></m-d-header>

These metrics measure of how well an embedding conserves expert-annotated biology.

1. **NMI cluster/label** - Normalized mutual information (NMI) compares the overlap of two clusterings. We use NMI to compare the cell type labels with an automated clustering computed on the integrated dataset (based on Louvain clustering). NMI scores of 0 or 1 correspond to uncorrelated clustering or a perfect match, respectively. Automated Louvain clustering is performed at resolution ranges from 0.1 to 2 in steps of 0.1, and the clustering output with the highest NMI with the label set is used.
2. **Cell type ASW** - The silhouette width measures the compactness of observations with the same labels. Averaging over all silhouette widths of a set of cells yields the average silhouette width (ASW), which ranges between -1 and 1. We use ASW to evaluate the compactness of cell types in the resulting embedding. The cluster ASW is computed on cell identity labels and scaled to a value between 0 and 1 using the equation:
<v-math>ASW = (ASW_{C}+1)/2 </v-math>
where <v-math>C</v-math> denotes the set of all cell identity labels.
3. **Cell cycle conservation** - The cell cycle conservation score is a proxy for the conservation of gene program signal during data integration. It evaluates how much variance is explained by cell cycle per batch before and after integration. This should ideally be equal. Using Scanpy’s `score_cell_cycle` function we score the cell cycle stage of each cell using the gene expression data and a gene set from [Tirosh et al. (10.1126/science.aad0501)](https://dx.doi.org/10.1126/science.aad0501).
We then compute the variance contribution of the resulting S and G2/M phase scores using principal component regression, which is performed for each batch separately. The differences in variance before, <v-math>Var_\text{before}</v-math>, and after, <v-math>Var_\text{after}</v-math>, integration is aggregated into a final score between 0 and 1, using the equation:

<v-math :inline="false">CC conservation = 1 -\frac{|Var_\text{after}-Var_\text{before}|}{Var_\text{before}}</v-math>

In this equation values close to 0 indicate lower conservation and 1 indicates complete conservation of the variance explained by the cell cycle. In other words, the variance remains unchanged within each batch for complete conservation, while any deviation from the pre-integration variance contribution reduces the score.
4. **Trajectory conservation** - The trajectory conservation score is a proxy for the conservation of a continuous biological signal in the joint embedding. In this metric, we compare trajectories computed after integration for relevant cell types that describe a continuous cellular differentiation process with a trajectory computed per batch and modality. Trajectories are computed using diffusion pseudotime (implemented in the `sc.tl.dpt` function in Scanpy). This approach embeds the data into a diffusion map space and computes an ordering of cells in this space from a selected root cell (a pseudotime value). As root cell, we select the cell in the earliest progenitor cluster that is most extremal in the first three diffusion components, which is still in the largest connected component of the cellular nearest neighbor graph (the graph that is used as the basis for the diffusion map computation).
The conservation of the trajectory is quantified via Spearman’s rank correlation coefficient, <v-math>s</v-math>, between the pseudotime values before and after integration. The final score is scaled to a value between 0 and 1 using the equation:
<v-math :inline="false">trajectory conservation = (s+1)/2.</v-math>
Values of 1 or 0 correspond to the same order of cells on the trajectory before and after integration or the reverse order, respectively. In cases where the trajectory could not be computed, which occurs when kNN graphs of the integrated data contain many connected components, we set the value of the metric to 0.
To compute a multimodal trajectory conservation score using un-modal ground-truth trajectories, we take the mean of the trajectory conservation scores for each modality.

<m-d-header text="Batch removal metrics" h=5 add-hash></m-d-header>

These metrics how well an embedding removes batch variation.

1. **Batch ASW** - As mentioned above, the average silhouette width (ASW) measures the compactness of observations with the same label in an embedding. We use the ASW to measure batch mixing by considering the non-compactness of batch labels per cell type cluster.
Specifically, we consider the absolute silhouette width, <v-math>s(i)</v-math>, on batch labels per cell i. Here, 0 indicates that batches are well mixed, and any deviation from 0 indicates a batch effect. We rescale this score so that higher scores indicate better batch mixing and compute this per cell type label, j, via the equation:
<v-math :inline="false">batchASW_{j} =\frac{1}{|C_{j}|}\sum_{i \in C_{j}} 1-|s(i)|</v-math>
where <v-math>C_j</v-math> is the set of cells with the cell label j and <v-math>|C_j|</v-math> denotes the number of cells in that set.
To obtain the final <v-math>batchASW</v-math> score, the label-specific <v-math>batchASW_j</v-math> scores are averaged:
<v-math :inline="false">batchASW =\frac{1}{|M|}\sum_{j \in M} batchASW_{j}</v-math>
Here, M is the set of unique cell labels. Overall, a batch ASW of 1 represents ideal batch mixing and a value of 0 indicates strongly separated batches.
<br style="margin-bottom: 10px;">
2. **Graph connectivity** - The graph connectivity metric assesses whether cells of the same type from different batches are close to one another in the embedding. This is evaluated by computing a k-nearest neighbour (kNN) graph, <v-math>G</v-math>, on the embedding using Euclidean distances. We then check if all cells with the same cell identity label are connected on this kNN graph. For each cell identity label <v-math>c</v-math>, we generate the subset kNN graph <v-math>G(N_c;E_c)</v-math>, which contains only cells from a given label. Using these subset kNN graphs, we compute the graph connectivity score:
<v-math :inline="false">gc =\frac{1}{|C|} \sum_{c \in C} \frac{|LCC(G(N_c;E_c))|}{|N_c|}</v-math>
Here, <v-math>C</v-math> represents the set of cell identity labels, <v-math>|LCC()|</v-math> is the number of nodes in the largest connected component of the graph, and <v-math>|N_c|</v-math> is the number of nodes with cell identity <v-math>c</v-math>. The resulting score has a range of <v-math>(0;1]</v-math>, where 1 indicates that all cells with the same cell identity are connected in the integrated kNN graph, and the lowest possible score indicates a graph where no cell is connected.

<m-d-header text="Metric aggregation" h=5 add-hash></m-d-header>

To rank methods, the individual metric scores will be aggregated. However, due to the differing nature of each metric, we will assign a weight to each metric after 1 month of the public competition. The goal of this weighting will be to provide equal importance on each measure when summing them. This weighting will be noted in the competition documentation and in communication to all competitions.

An overall weighted average of batch correction and bio-conservation scores will be computed via the equation:
<v-math :inline="false">S_{overall,i} = 0.6 \cdot S_{bio,i} + 0.4 \cdot S_{batch,i}</v-math>
This reflects the relative importance of the metrics.

The batch covariate used for evaluation is `sample`, however one can consider encoding the nested donor and site of data collection as a nested batch covariate.

Further information on any of these metrics can be found at https://www.biorxiv.org/content/10.1101/2020.05.22.111161v2


<m-d-header text="Prizes" h=3 add-hash alternate="prizes-3"></m-d-header>


Because labels used for the metric calculations are available for some of the data as described in the [Benchmark Dataset](/neurips_docs/data/dataset/) notes, we anticipate a bias in performance in models that use this information in model training. As there is no way for us to distinguish between pre-trained models that use these labels and those that don't, we are splitting the prizes into two categories.

**Pre-trained models** are any method that includes model parameters through the `resources` block of the `config.vsh.yaml` file. For more information, see the [FAQs](/neurips_docs/submission/faq/#how-can-i-upload-a-pre-trained-model). This also includes models that may download model weights during execution.

**Non pre-trained models** are any method that uses only the `input_mod[1|2]` files to generate the embedding.

For this task, four &dollar;1250 prizes will be awarded to the submissions for each of the following criteria:
1. Best performance embedding GEX and ATAC using a pre-trained model
2. Best performance embedding GEX and ADT using a pre-trained model
1. Best performance embedding GEX and ATAC without pre-training
2. Best performance embedding GEX and ADT without pre-training

<m-d-header text="Data" h=1 add-hash></m-d-header>

The goal for this page is to provide an introduction to multimodal single-cell data and the specific datasets used in the competition. We know that many participants may be encountering this data type for the first time (it's only been around for a couple of years!), and so this page serves as a general introduction. At the end you will find further reading recommendations that you can consult if you'd like to learn more.

<m-d-header text="The flow of genetic information in cells" h=2 add-hash></m-d-header>

Cells are the fundamental unit of life. All living things are made of of cells. This includes trees, fish, humans, bacteria, fungi, etc. Cells come in all shapes and sizes, but they have several properties in common no matter where we look. For example, all cells are membrane-bound. This means they have a clear boundary between inside and outside. All cells contain some form of organelles, which are specialized substructures that perform a specific function. And, all cells contain three layers of genetic information: [DNA](https://www.genome.gov/genetics-glossary/Deoxyribonucleic-Acid), [RNA](https://www.genome.gov/genetics-glossary/RNA-Ribonucleic-Acid), and [protein](https://www.genome.gov/genetics-glossary/Protein).


<figure width="100%">
  <img width="100%" src="/media/learning/central-dogma-large.png">
  <figcaption>
    <p style="font-size: medium;">
      <strong>Genetic information within a cell flows from DNA to RNA to protein</strong>
    </p>
  </figcaption>
</figure>

DNA defines an organism. Indeed, you can change the species of a bacteria through [genome transplantation](https://pubmed.ncbi.nlm.nih.gov/17600181/). However, DNA is not functional. It contains a set of instructions that must be converted into RNA and then into protein. For most purposes, you can think of RNA as a messenger between DNA and protein. DNA is made up of [genes](https://www.genome.gov/genetics-glossary/Gene) that contain instructions on how to make proteins. Proteins are responsible for carrying out biological functions in the cell, such as metabolising glucose to create energy for the cell.  Generally speaking, each protein in your body is encoded by a single gene.

Although all of cells in your body contain the same genome, the same set of DNA, these trillions carry out very different biological functions. The differences between an immune cell, a neuron, or a muscle cell is defined by which genes are turned on or off within those cells. When a gene is turned on, more copies of RNA are created, thereby increasing the production of protein. We know that regulation of the amount of protein both happens at the level of transcription (DNA -> RNA) and translation (RNA -> protein).

<figure style="display:inline-flex">
  <img style="width: 500px; display:inline-flex; align-self:center" src="/media/learning/differing_expression.png">
  <figcaption>
    <p style="font-size: medium;">
      <strong>Regulation of gene expression affects the amount of RNA and protein in the cells</strong>. In this example, gene A is more upregulated than gene B resulting more RNA and more protein.
    </p>
  </figcaption>
</figure>

Because we know that the difference between types of cells has to do with different levels of RNA and proteins, it's very useful to be able to measure the abundance of these molecules at the level of individual cells. Not only does this give us a fine-resolution view into the different kinds of cells in the body, it also provides insight into how the same set of DNA instructions can be interpreted so differently throughout the body.

The promise of single-cell measurements of genetics information is that by better understanding how this information flow within our cells and tissues, we might better understand what goes wrong in the context of disease.

<m-d-header text="A rough history of single-cell technologies" h=2 add-hash></m-d-header>

This is an exciting time to study single-cell data.

<figure>
  <img width="100%" src="/media/learning/timeline.png">
  <figcaption>
    <p style="font-size: medium;">
      <strong>An abbreviated timeline of single-cell technologies</strong>
    </p>
  </figcaption>
</figure>

The first measurement of RNA from single cells was described in Eberwine et al. [(1992)](https://www.pnas.org/content/89/7/3010) using molecular probes. It wasn't until [2009](https://www.ncbi.nlm.nih.gov/pubmed/19349980/), when Tang et al. described the sequencing of the transcriptome of a single cell (a mouse blastomere). In the following 6 years, several innovations were developed to improve the throughput of single-cell RNA sequencing (scRNA-seq). Perhaps the most impactful was the simultaneous description of two droplet-based protocols for capturing single cells into nanoliter droplets in oil emulsion described by Klein et al. [(2015)](https://pubmed.ncbi.nlm.nih.gov/26000487/) and Macosko et al. [(2015)](https://pubmed.ncbi.nlm.nih.gov/26000488/). With droplet-based single-cell methods, it became possible to perform experiments with tens of thousands of cells.

<figure>
  <img width="100%" src="/media/learning/dropseq.gif">
  <figcaption>
    <h4>
      Capture of a single cell in a nanoliter droplet
    </h4>
    <p style="font-size: medium;">
      In this video from <a href="https://dropseq.org">dropseq.org</a>, we see a single cell (bottom left channel) captured in a nanoliter droplet with an oligo-coated bead (left channel) in an oil emulsion (top and bottom right channels).
    </p>
  </figcaption>
</figure>


The next major innovation in single-cell measurement  came in Steockius et al. [(2017)](https://pubmed.ncbi.nlm.nih.gov/28759029/) and Cao et al. [(2018)](https://science.sciencemag.org/content/361/6409/1380) with the introduction of multimodal single-cell data measuring both RNA and protein or chromatin accessibility and RNA, respectively. The first method, called Cellular Indexing of Transcriptomes and Epitopes by Sequencing (CITE-seq), measures all RNA and expression of ~10-200 cell surface markers in individual cells. The second method, referred to as Multiome Assay for Transposase-Accessible Chromatin using sequencing (ATAC) + Gene Expression, provides a measure of chromatin accessibility throughout the genome and levels of gene expression for all genes. Unlike CITE-seq, this Multiome technology provides a view across all DNA and RNA in the cell. The following year, Nature Methods named Single-cell multimodal omics the [Method of the Year 2019](https://www.nature.com/articles/s41592-019-0703-5) noting these methods "opened unprecedented opportunity for the development of advanced statistical and computational methods."

We are excited that in 2021, there are robust commercially-available reagents that we can use to create a reference benchmarking dataset to drive innovation in multimodal single-cell data integration.

<m-d-header text="Measuring multiple modalities in single cells" h=2 add-hash></m-d-header>

We know that DNA must be [accessible](https://www.nature.com/articles/s43586-020-00008-9) (ATAC data) to produce mRNA (expression data), and mRNA in turn is used as a template to produce protein (protein abundance). These processes are regulated often by the same molecules that they produce: for example, a protein may bind DNA to prevent the production of more mRNA. Understanding these regulatory processes would be transformative for synthetic biology and drug target discovery. Any method that can predict a modality from another must have accounted for these regulatory processes, but the demand for multi-modal data shows that this is not trivial.

<figure style="width:100%;display:inline-flex;">
  <img style="width:100%; max-width:800px; min-width:50%;display:inline-flex;align-self:center;" src="/media/learning/ATAC-seq.svg">
  <figcaption>
    <h3>
      Multimodal scRNA and scATAC from cell nuclei
    </h3>
    <p style="font-size: medium;">
      With the 10X Genomics Single-Cell Multiome ATAC + Gene Expression kit, it is possible to measure chromatin accessibility and RNA expression in tens of thousands of cells. These methods only measure RNA within the nucleus of the cell.
    </p>
  </figcaption>
</figure>

It is also powerful to be able to capture both RNA and protein expression in the same cell. Proteins on the cell surface are not only important for identifying different cell populations, but these proteins also serve functional roles, especially within the immune system. Currently, it is only possible to measure roughly 100-200 proteins on an individual cell, but we know cells contain tens of thousands (or more) unique proteins. Nevertheless, this information has shown crucial for [disentangling](https://www.cell.com/cell/fulltext/S0092-8674(21)00583-3) the identities of cell populations that are transcriptionally similar but express different functional surface markers.


<figure style="width:100%;display:inline-flex;">
  <img style="width:100%; max-width:800px; min-width:50%;display:inline-flex;align-self:center;" src="/media/learning/CITE-seq.svg">
  <figcaption>
    <h3>
      Multimodal scRNA and protein abundance from individual cells
    </h3>
    <p style="font-size: medium;">
      CITE-seq provides a measure of gene expression at the level of RNA and measure of protein abundance for 10-200 cell surface proteins.
    </p>
  </figcaption>
</figure>

<m-d-header text="Multimodal data as a basis for benchmarking" h=2 add-hash></m-d-header>

Developing machine learning methods for biological systems is complicated by the difficulty of obtaining ground truth. Typically, machine learning tasks rely on manual annotation (as in images or natural language queries), dynamic measurements (as in longitudinal health records or weather), or multimodal measurement (as in translation or text-to-speech). However, this is more complicated in the context of single-cell biology.

With single-cell data, annotation isn't feasible. The data is noisy and not fully understood with descriptions of cell types evolving rapidly. Similarly, longitudinal measurement of all the RNA in a cell isn't possible because the current measurement technologies involve destroying the cell. However, with multimodal single-cell data, we can now directly observe two layers of genetic information in the same cells. This provides an opportunity to use the fact these two sets of data were observed co-occurring in the same cells as ground truth. This is akin to the way that access to the same sentiment expressed in two languages provides ground truth for machine translation.

However, as these technologies are relatively new, most publicly available datasets are designed for exploration, not benchmarking. To set up a competition for multimodal single-cell data integration, we set out to create a fit-for-purpose benchmarking dataset.


<m-d-header text="Benchmarking" h=1 add-hash></m-d-header>

<info-box>
All the training data is released! Due to supply chain issues, we've had to deviate from the planned study design for Site 3 Multiome.

The study design is as follows:

**Multiome**
* Site 1 - Donors 1, 2, 3
* Site 2 - Donors 1, 4, 5
* Site 3 - Donors 3, 6, 7, 10
* Site 4 - Donors 1, 8, 9

**CITE**
* Site 1 - Donors 1, 2, 3
* Site 2 - Donors 1, 4, 5
* Site 3 - Donors 1, 6, 7
* Site 4 - Donors 1, 8, 9
</info-box>

We are currently in the process of creating a benchmarking dataset for the competition. There will be two types of data:
* Joint profiling of single-cell RNA and protein using the [10X Genomics
Single Cell Gene Expression with Feature Barcoding](https://support.10xgenomics.com/single-cell-gene-expression/overview/doc/getting-started-single-cell-gene-expression-with-feature-barcoding-technology) with the  [Biolegend TotalSeq™-B Universal Cocktail v1.0](https://www.biolegend.com/en-us/products/totalseq-a-human-universal-cocktail-v1-0-20321?GroupID=GROUP28) panel  
* Joint profiling of single-nucleus RNA and chromatin accessibility using the [10X Genomics Single Cell Multiome ATAC + Gene Expression Kit](https://www.10xgenomics.com/products/single-cell-multiome-atac-plus-gene-expression)  



<m-d-header text="Download the data" h=2 add-hash></m-d-header>

The dataset is available from NCBI GEO under accession [GSE194122](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE194122).

In this series you will find BAM files containing the raw sequencing data and proccess and curated datasets stored in the H5AD file format. The processed data files are all [AnnData](https://anndata.readthedocs.io/en/latest/) objects, as described in the following section.

<m-d-header text="Data file format" h=2 add-hash></m-d-header>

The training data is accessible in an [AnnData](https://anndata.readthedocs.io/en/latest/) h5ad file. More information can be found on AnnData objects [here](/neurips_docs/submission/quickstart/). You can load these files is to use the `AnnData.read_h5ad()` function. The easiest way to get started is to [spin up a free Jupyter Server on Saturn Cloud](/neurips_docs/about/explore).

```python
!pip install anndata
import anndata as ad

adata_gex = ad.read_h5ad("cite/cite_gex_processed_training.h5ad")
adata_adt = ad.read_h5ad("cite/cite_adt_processed_training.h5ad")
```

You can find code examples for exploring the data in our data [exploration notebooks](https://github.com/openproblems-bio/neurips2021-notebooks/tree/main/notebooks/explore).

<m-d-header text="Preprocessing" h=2 add-hash></m-d-header>

To facilitate exploring the data, each dataset has been preprocessed to remove low quality cells and doublets. The following sections detail this process for each data modality.

<m-d-header text="Preprocessing of gene expression (GEX)" h=3 add-hash></m-d-header>

In this dataset, gene expression was measured using 3' capture of nuclear RNA as described in the [10X Multiome Product Guide](https://www.10xgenomics.com/products/single-cell-multiome-atac-plus-gene-expression). Note, not all RNA is found in the nucleus. Comparisons of nuclear and cytosolic RNA have been previously reported (e.g. [Bakken 2018](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0209648); [Abdelmoez 2018](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-018-1446-9)) as have comparisons of single-nucleus and single-cell RNA sequencing ([Lake 2017](https://www.nature.com/articles/s41598-017-04426-w)).

For gene expression data, cells were filtered based on mitochondrial content, UMI counts per cell, and genes detected per cell. Size factors were then calculated using [scran](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0947-7) and stored in `adata.obs["size_factors"]`.

Counts were then normalized per cell by divided the UMI counts by the size factors. Original counts are stored in `adata.layers["counts"]`. The size factor normalized counts are stored in `adata.X`.

Finally, normalized counts are [log1p transformed](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.log1p.html). These normalized counts are stores in `adata.layers["log_norm"]`.

More information about best practices for single-cell analysis can be found [here](https://www.embopress.org/doi/full/10.15252/msb.20188746).

<m-d-header text="Preprocessing of ATAC" h=3 add-hash></m-d-header>

The chromatin accessibility data acquired by ATAC-seq as part of the 10X Multiome protocol was processed using [Signac](https://satijalab.org/signac/). Quality control, dimensionality reduction and translating peaks to gene activity scores was performed using Signac, following the authors' instructions. After loading the peak-by-cell matrix, counts were binarized to only represent an accessible versus non-accessible state of each region. Cells were then filtered based on 5 quality control metrics comprising the total number of fragments, the enrichment of fragments detected at transcription start sites (TSS), the fraction of fragments in peak regions compared to peak-flanking regions, the fraction of peaks blacklisted by the ENCODE consortium, and the nucleosome signal, which describes the length distribution of fragments which is expected to follow the length of DNA required span across one nucleosome or multiples of it.

Since ATAC data is sparser than gene expression data, peaks were included if they were accessible in at least 15 cells.

Finally, the data was binarized by setting all values `>0` to `1` and stored in `adata.X`. Raw UMI counts for each peak can be found in `adata.layers["counts"]`.

<m-d-header text="Preprocessing of protein abundance (ADT)" h=3 add-hash></m-d-header>

The protein data was measured using the [TotalSeq™-B Human Universal Cocktail, V1.0](https://www.biolegend.com/en-us/products/totalseq-b-human-universal-cocktail-v1dot0-20960) of 134 cell surface markers and 6 isotype controls. The isotype controls are stored in `adata.obsm["isotype_controls"]`. These controls do not target any human proteins and their expression should be considered background.

The ADT protein measurements were run through quality control based on the total number of ADTs (ranging from 1100-1200 to 24000 across samples), the number of proteins captured in each cell (with a lower limit of 80) and the ADT count of the 6 isotype controls summed up in each cell (ranging from 1 to 100).

Since the total number of captured ADTs is limited, absolute ADT counts appear to be lower if highly abundant proteins are present. To account for this effect, normalization was performed using the centered log ratio (CLR) transformation. CLR counts are stored in `adata.X` and the raw counts are stored in `adata.layers["counts"]`.

<m-d-header text="Metadata" h=2 add-hash></m-d-header>

More information about the features are available in the `.var` and `.obs` DataFrames of each object.


<m-d-header text="Gene expression observation metadata" h=3 add-hash></m-d-header>


The GEX `adata` objects have the following columns:
* `.obs.index` - The cell barcode for that observation with the batch label appended.
* `.obs["n_genes_by_counts"]` - The number of genes with at least 1 count in a cell.
* `.obs["pct_counts_mt"]` - Percent of UMI counts mapped to mitochondrial genes.
* `.obs["n_counts"]` - Number of UMIs detected in the cell
* `.obs["n_genes"]` - Number of genes detected in the cell
* `.obs["size_factors"]` - The estimated size factor for the cell. See [OSCA Ch. 7 - Normalization](https://bioconductor.org/books/release/OSCA/normalization.html)
* `.obs["phase"]` - The [cell cycle](https://www.genome.gov/genetics-glossary/Cell-Cycle) phase for each cell as calculated by [scanpy.tl.score_genes_cell_cycle](https://scanpy.readthedocs.io/en/stable/generated/scanpy.tl.score_genes_cell_cycle.html)
* `.obs["leiden_final"]` -
* `.obs["atac_ann"]` - The cell type annotation of the cell from the joint ATAC data
* `.obs["cell_type"]` - The cell type annotation of the cells from the GEX data
* `.obs["pseudotime_order_GEX"]` - The [diffusion pseudotime](https://www.nature.com/articles/nmeth.3971) annotation for the developmental trajectories annotated in the data.
* `.obs["batch"]` - The batch from which the cell was sampled. Format is `s1d1` for Site 1 Donor 1.

For more info on how the QC metrics were calculated, consult [scanpy.pp.calculate_qc_metrics](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.calculate_qc_metrics.html)

<m-d-header text="Gene expression feature metadata" h=3 add-hash></m-d-header>

The GEX `adata.var` DataFrames have the following columns:

* `.var.index` - [Ensembl Gene Names](https://m.ensembl.org/info/genome/genebuild/gene_names.html) for each gene
* `.var["gene_ids"]` - [Ensembl Stable IDs](https://useast.ensembl.org/info/genome/stable_ids/index.html) used to uniquely track genes whose Gene Names may change over time.
* `.var["feature_types"]` - Denotes the each feature as a gene expression feature. Should be `GEX` for all genes
* `.var["genome"]` - The [Genome Assembly](https://www.ncbi.nlm.nih.gov/assembly/GCF_000001405.26/) used for read mapping.
* `.var["n_cells-[batch]"]` - The number of cells in `[batch]` in which the gene was detected.
* `.var["highly_variable-[batch]"]` - Whether the gene was determined to be [highly variable](https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.highly_variable_genes.html) in `[batch]`

<m-d-header text="ATAC observation metadata" h=3 add-hash></m-d-header>

The ATAC `adata.obs` DataFrames have the following columns:
* `.obs.index` - The cell barcode for that observation with the batch label appended.
* `.obs["nCount_peaks"]` - The number of peaks detected in the cell.
* `.obs["atac_fragments"]` - Number of UMI counts in the cell (both in and not in peaks)
* `.obs["reads_in_peaks_frac"]` - Fraction of UMIs in peaks
* `.obs["blacklist_fraction"]` - Fraction of UMIs in [Encode Blacklisted](https://www.nature.com/articles/s41598-019-45839-z) regions
* `.obs["nucleosome_signal"]` - The [nucleosome signal](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-1929-3#Sec17), which describes the length distribution of fragments which is expected to follow the length of DNA required span across one nucleosome or multiples of it
* `.obs["phase"]` - The [cell cycle](https://www.genome.gov/genetics-glossary/Cell-Cycle) phase for each cell as calculated by [scanpy.tl.score_genes_cell_cycle](https://scanpy.readthedocs.io/en/stable/generated/scanpy.tl.score_genes_cell_cycle.html)
* `.obs["leiden_final"]` -
* `.obs["rna_ann"]` - The cell type annotation of the cell from the joint RNA data
* `.obs["cell_type"]` - The cell type annotation of the cells from the ATAC data
* `.obs["pseudotime_order_ATAC"]` - The [diffusion pseudotime](https://www.nature.com/articles/nmeth.3971) annotation for the developmental trajectories annotated in the data.
* `.obs["batch"]` - The batch from which the cell was sampled. Format is `s1d1` for Site 1 Donor 1.

For more info on how the QC metrics were calculated, consult the [Signac documentation](https://satijalab.org/signac/).

<m-d-header text="ATAC feature metadata" h=3 add-hash></m-d-header>

The ATAC `adata.var` DataFrames have the following columns:

* `.var.index` - [Genomic coordinates](https://www.idtdna.com/pages/support/faqs/how-are-genomic-coordinates-defined) for each [ATAC peak](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-1929-3) that are directly related to the reference genome, and include the chromosome name*, start position, and end position in the following format: `chr1-1234570-1234870`.
* `.var["feature_types"]` - Denotes the each feature as a gene expression feature. Should be `ATAC` for all peaks
* `.var["n_cells-[batch]"]` - The number of cells in `[batch]` in which the peak was detected.

*For the curious, chromosome names like `KI270726.1` represent scaffold that are either unlocalized or unplaced (see [Genome Assemblies from Ensembl](https://grch37.ensembl.org/info/genome/genebuild/assembly.html))

There is also information about the observations in the `.obs` DataFrame of each AnnData object.

<m-d-header text="Submission" h=1 add-hash></m-d-header>
<m-d-header text="Development Process" h=2 add-hash></m-d-header>

The typical workflow for developing a new method and submitting the results to the EvalAI repository is described below.

<m-d-header text="Editing the script and config file" h=3 add-hash></m-d-header>

Implement your method in `script.py` (or `script.R` for R users) and update the Viash config accordingly. The file format of the script and config are described in the [Starter kit contents](/neurips_docs/submission/starter_kit_contents) description.

<m-d-header text="Manually running the component" h=3 add-hash></m-d-header>

The easiest way to run a viash component is by using the command-line interface. You can view a component's interface using the `--help` flag:

```bash
$ bin/viash run config.vsh.yaml -- --help
python_starter_kit dev
A description for your method.

Options:
   --input_train_mod1
        type: file, required parameter
        example: dataset_mod1.h5ad
        Censored dataset, training cells.

   --input_test_mod1
        type: file, required parameter
        example: dataset_mod1.h5ad
        Censored dataset, test cells.

...
```

To run the component, simply provide parameters required by the component:

```bash
$ DATA_PATH="sample_data/openproblems_bmmc_cite_starter/openproblems_bmmc_cite_starter"
$ bin/viash run config.vsh.yaml -- \
  --input_train_mod1 "${DATA_PATH}.train_mod1.h5ad" \
  --input_train_mod2 "${DATA_PATH}.train_mod2.h5ad" \
  --input_test_mod1 "${DATA_PATH}.test_mod1.h5ad" \
  --output "output.h5ad"
```

Behind the screens, Viash will run your code inside a Docker container where the input data is automatically mounted. 

Alternatively, for debugging purposes, you can also run the script manually by running `python script.py` (or `Rscript script.R` for R users). Note that this bypasses any containerization or code generation functionality provided by Viash.

<m-d-header text="Unit test the component" h=3 add-hash></m-d-header>
If you can run the script directly, you should next proceed to running unit tests on the data.

From within the starter kit, you can run the code on the sample dataset as follows:

```sh
$ ./scripts/1_unit_test.sh
```

<m-d-header text="Generating a submission" h=3 add-hash></m-d-header>

If you can run your contribution on sample data, you can now proceed to generating a submission file.

```sh
$ ./scripts/2_generate_submission.sh
```

If this process runs successfully, then you will be instructed to upload the submission to EvalAI. If you see any warnings, please consult our [FAQ](/neurips_docs/submission/faq).

<m-d-header text="Evaluate locally" h=3 add-hash></m-d-header>

You can evaluate the results of your submission using the included evaluation script. Note this must be run after generating a submission.

```sh
$ ./scripts/3_evaluate_submission.sh
```

The evaluation results will be saved at a path like `output/evaluation/predict_modality`.

<m-d-header text="Advanced topics" h=3 add-hash></m-d-header>
<m-d-header text="Use additional files" h=4 add-hash></m-d-header>


For your component to be able to access additional files (such as a pre-trained model) from within your script, you need to specify these files under `.functionality.resources` in the config script as follows:
```yaml
  # files your script needs
  resources:
    # the script itself
    - type: python_script
      path: script.py
    # additional resources your script needs (optional)
    - path: pretrained_model.pt
```

For Python users, you can access the file at `meta['resources_dir'] + '/pretrained_model.pt'`. Similarly, R users can access the script at `paste0(meta$resources_dir, "/pretrained_model.pt")`.

<m-d-header text="Troubleshooting" h=3 add-hash></m-d-header>


If you encounter any issues, please look at the [Frequently Asked Questions](/neurips_docs/submission/faq). If this doesn't solve your problem, visit the #support channel on Discord.


<m-d-header text="FAQ" h=1 add-hash></m-d-header>

Here are a list of common questions and answers related to the competition
<m-d-header text="What if I only want to compete for one of the prizes in a task?" h=2 add-hash></m-d-header>

As described in the Tasks documentation, each task has up to five prizes. However, competitors need not compete for all prizes in a task. In Task 1 - Modality Prediction, for example, a competitor may compete on the subtask of predicting RNA from ATAC measurements. In this case, a submitted method may simply exit without writing a submission to disk.

<m-d-header text="Can I store helper functions as separate files?" h=2 add-hash></m-d-header>


Yes, though you'll need to let Viash know which additional files are required to run the component. If several helper functions are stored in an additional file `mymodule.py` or `mymodule.R`, use the following code to import helper functions:

<m-d-header text="Python" h=3 add-hash></m-d-header>

In the functionality resources section in the config:

```yaml
resources:
  - type: python_script
    path: script.py
  - path: mymodule.py
```

In the main Python script:

```python
import sys

## VIASH START
meta = { 'resources_dir': '.' }
## VIASH END

sys.path.append(meta['resources_dir'])
from mymodule import helper_fun
```

<m-d-header text="R" h=3 add-hash></m-d-header>


In the functionality resources section in the config:

```yaml
resources:
  - type: r_script
    path: script.R
  - path: mymodule.R
```

In the main R script:

```r
## VIASH START
meta <- list(resources_dir = ".")
## VIASH END

source(paste0(meta[["resources_dir"]], "/mymodule.R"))
helper_fun(...)
```
<m-d-header text="How can I upload a pre-trained model?" h=2 add-hash></m-d-header>

Competitors may submit pre-trained models for any of the tasks in the competition. Model parameters may included in the submission directory. They can be then made accessible to the submission script by editing the `config.vsh.yaml` file to list the parameter file under the `resources` section.

For example if you'd like to add a file containing model weights with the filename `weights.pt`, edit the resources block to look like the following:
```yaml
resources:
  # Script containing method
  - type: python_script
    path: script.py
  # Model weights file
  - type: file
    path: weights.pt
```

This file will now be made accessible to the script using the "resources directory". You can load the file as follows:

```python
## VIASH START
# ...
meta = { 'resources_dir': '.' }
## VIASH END

torch.load(meta['resources_dir'] + '/weights.pt')
```

For more information, see [Updating the Configuration](/neurips_docs/submission/starter_kit_contents/#updating-the-configuration).

<m-d-header text="Can I pre-train on public data?" h=2 add-hash></m-d-header>


Pre-training on public data is allowed. We've already compiled a large number of public datasets [here](https://github.com/openproblems-bio/neurips2021_multimodal_viash/tree/main/src/common/datasets). Note, these datasets are not all filtered, preprocessed, and annotated in the same way as the competition training and test data. We have no prior expectation about whether including public data will or will not improve performance on the in house test data. Use of public data is at the competitors risk.

<m-d-header text="How are the libraries in config.vsh.yaml installed?" h=2 add-hash></m-d-header>


If you'd like to see how the viash docker image is built, run `bin/viash run -- ---dockerfile`. Here's an example from the predict modality starter kit.

```sh
> bin/viash run -- ---dockerfile

FROM dataintuitive/randpy:py3.8

RUN pip install --upgrade pip && \
  pip install --no-cache-dir "scikit-learn" "anndata" "scanpy"
```

Here you can see the base Docker image is https://hub.docker.com/r/dataintuitive/randpy at the [py3.8](https://hub.docker.com/layers/dataintuitive/randpy/py3.8/images/sha256-21c7d4fb8ecf787040590b62753fb1439022e706800cde110f7d20c1fdccaab3?context=explore) tag.



<m-d-header text="Error message 'Process terminated with an error exit status (xxx)'" h=2 add-hash></m-d-header>


When running a Nextflow pipeline, it's possible your component might fail when running on one of multiple datasets. Here is an example output of a nextflow execution that failed:

```sh
$ ./scripts/2_generate_submission.sh

N E X T F L O W  ~  version 21.04.1
Pulling openproblems-bio/neurips2021_multimodal_viash ...
Launching `openproblems-bio/neurips2021_multimodal_viash` [small_montalcini] - revision: 24adec7995 [1.1.1]

...
[5f/5ff487] process > method:method_process (openproblems_bmmc_cite_phase1) [100%] 2 of 2, failed: 1 ✔
[e2/7371b2] NOTE: Process `method:method_process (openproblems_bmmc_multiome_phase1)` terminated with an error exit status (1) -- Error is ignored
Completed at: 24-Sep-2021 09:33:36
Duration    : 1m 27s
CPU hours   : 0.1 (5.7% failed)
Succeeded   : 1
Ignored     : 1
Failed      : 1
```

Pay attention to the exit status as well as the number of succeeded instances. If you managed to generate at least one output file (i.e. Succeeded > 0), you can still submit your solutions to eval.ai but will only get scored on the solutions you submitted.

<m-d-header text="View exit codes" h=3 add-hash></m-d-header>

The error notifications might have disappeared by the time the pipeline has finished running. Use the `nextflow log` command to view the hash codes and exit statuses of the different executions.

```
$ bin/nextflow log small_montalcini -f hash,name,exit,status
5f/5ff487	  method:method_process (openproblems_bmmc_cite_phase1)	      0   COMPLETED
e2/7371b2	  method:method_process (openproblems_bmmc_multiome_phase1)	  1   FAILED
```
<m-d-header text="Interpret exit codes" h=3 add-hash></m-d-header>


The reason why an execution failed can often be derived from the exit code:

* 1: An exception occurred from within the script. See the relevant section below.
* 127: The Docker container could not be built. Rerunning the submission script might help, otherwise contact the #support channel in Discord.
* 137: The process ran out of memory. See the relevant section below.

<m-d-header text="Solving exit code 1" h=3 add-hash></m-d-header>

The first step in finding out what went wrong with this execution is to check the Nextflow work directory. This contains all the information that was generated
throughout the process. Note that the Nextflow log only shows the first few characters of the hashcode, so you will need to use autocomplete to get the full path name.


```sh
$ ls -a1 work/e2/7371b25a57ecc11946346b462e7d2f/

.command.begin
.command.err
.command.log
.command.out
.command.run
.command.sh
.exitcode
openproblems_bmmc_multiome_phase1.censor_dataset.output_mod1.h5ad
openproblems_bmmc_multiome_phase1.censor_dataset.output_mod2.h5ad
```


<info-box>
**Tip:** Modify the command above depending on the output you got from Nextflow. For example, if Nextflow tells you process `5f/5ff487` failed, you should be looking in the `work/5f/5ff487...` directory and not the `work/e2/7371b2..` directory. Use auto-completion to get the full name of the directory as Nextflow only displays the first 6 characters hash.
</info-box>

You can view the `exitcode`, `stdout` and `stderr` of this process by viewing the `.exitcode`, `.command.log` and `.command.err` files, respectively.

In this case, an exception was thrown after the data was loaded because the method at hand is specifically designed for GEX+ADT and not GEX+ATAC data.

```
$ cat work/e2/7371b25a57ecc11946346b462e7d2f/.exitcode
1

$ cat work/e2/7371b25a57ecc11946346b462e7d2f/.command.log 
Loading dependencies
Loading datasets
Error: this method only works on GEX+ADT data
Execution halted

$ cat work/e2/7371b25a57ecc11946346b462e7d2f/.command.err 
Error: this method only works on GEX+ADT data
Execution halted
```

If the error log is not sufficient in figuring out the issue, we suggest debugging your script by editing the viash codeblock and running through your code step by step.
To do this, change the paths specified as follows (example shown for Python but analagous in R):

```python
## VIASH START
par = {
    'input_mod1' : "output/datasets/joint_embedding/openproblems_bmmc_multiome_phase1/openproblems_bmmc_multiome_phase1.censor_dataset.output_mod1.h5ad",
    'input_mod2' : "output/datasets/joint_embedding/openproblems_bmmc_multiome_phase1/openproblems_bmmc_multiome_phase1.censor_dataset.output_mod2.h5ad",
    'output' : "debug_output.h5ad",
    # ... other parameters
}
## VIASH END
```

<m-d-header text="Solving exit code 137" h=3 add-hash></m-d-header>

Exit code 137 means that one of the instances where your script was ran on one of the datasets ran out of memory (max 10GB by default).

If you're using Docker Desktop on Mac OS X, a common cause is the default memory constraint being 2GB. To increase the memory constraint, please edit the [Resources Configuration](https://docs.docker.com/desktop/mac/#resources).

If this isn't the issue, your script is simply using too much memory. By default, methods get 10GB to run on one of the datasets. Try manually running the code blocks in your script and try to optimize where possible:

* Remove large data objects when not being used anymore
* Use sparse data matrices whenever possible
* Use algorithms with a lower algorithmic complexity

If none of the options above are possible, consider upgrading the memory limits of your component, as documented below.


<m-d-header text="How can I increase the memory/CPU/runtime limits of my method?" h=2 add-hash></m-d-header>

The resource use of the submission components is set through the `config.vsh.yaml` file available in the starter kits.

```yaml
# By specifying a 'nextflow', viash will also build a viash module
# which uses the docker container built above to also be able to
# run your method as part of a nextflow pipeline.
- type: nextflow
  labels: [ lowmem, lowtime, lowcpu ]
```

Available options are `[low|med|high]` for each of `mem`, `time`, and `cpu`. The corresponding resource values can be found in the `scripts/nextflow.config` file.

<m-d-header text="My submission is stuck at status 'Submitted'" h=2 add-hash></m-d-header>

This status means your submission has been submitted to the queue but hasn't been picked up by the evaluation worker yet.
Depending on how many submissions are being submitted by yourself and other competitors, a delay of about 30 minutes is expected.
If you're experiencing longer waiting times, please contact @rcannood in the Discord #support channel.

<m-d-header text="How does the Nextflow execution in 2_generate_submission.sh work?" h=2 add-hash></m-d-header>

The codeblock which executes the actual execution of your method on each of the datasets is the following:

```bash
bin/nextflow \
  run openproblems-bio/neurips2021_multimodal_viash \
  -r $PIPELINE_VERSION \
  -main-script src/predict_modality/workflows/generate_submission/main.nf \
  --datasets 'output/datasets/predict_modality/**.h5ad' \
  --publishDir output/predictions/predict_modality/ \
  -resume \
  -latest \
  -c scripts/nextflow.config
```

You can split up the command above as follows:

* Starting a Nextflow pipeline:
  ```
  bin/nextflow run
  ```
* Specify where the pipeline is located:
  ```
  openproblems-bio/neurips2021_multimodal_viash -r 1.2.0
  ```
* Specify the path of the pipeline script within the repository:
  ```
  -main-script src/predict_modality/workflows/generate_submission/main.nf
  ```
* Path to datasets:
  ```
  --datasets 'output/datasets/predict_modality/**.h5ad'
  ```
* Path to output:
  ```
  --publishDir output/predictions/predict_modality/
  ```
* Resource parameter file:
  ```
  -c scripts/nextflow.config
  ```
* Resume on previous executions:
  ```
  -resume
  ```
* Always pull latest GitHub repository if possible:
  ```
  -latest
  ```

The pipeline script (`src/predict_modality/workflows/generate_submission/main.nf`) contains more or less the following code:

```nextflow
nextflow.enable.dsl=2

include { method } from "$launchDir/target/nextflow/main.nf" params(params)

params.datasets = "s3://neurips2021-multimodal-public-datasets/predict_modality/**.h5ad"

workflow {
  main:
  print(params.datasets)
  Channel.fromPath(params.datasets)
    | map { [ it.getParent().baseName, it ] }
    | filter { !it[1].name.contains("output_test_mod2") }
    | groupTuple
    | map { id, datas -> 
      def fileMap = datas.collectEntries { [ (it.name.split(/\./)[-2].replace("output_", "input_")), it ]}
      [ id, fileMap, params ]
    }
    | method
}
```

This script might me a little hard to read if you don't know any Nextflow DSL or Groovy, but it's actually rather simple. This script:

* looks in the path specified by the --datasets parameter, which is a list of all h5ad files in the output/datasets/predict_modality folder. 
* it maps the files to a tuple [ parent dir name, file ]
* it filters away files containing the term output_test_mod2 because these are the solution files
* it groups the files by the directory name (name of the dataset)
* it transforms the list of tuples to the correct parameter names, e.g. [ dataset_id, [ input_train_mod1: file, input_train_mod2: file, input_test_mod1: file ], params ]
* runs the nextflow module generated by viash

<m-d-header text="Can I use an Nvidia GPU to train the model" h=2 add-hash></m-d-header>

Absolutely! The evaluation worker runs on an Amazon EC2 G4dn instance, so it has 1 NVIDIA T4 GPU available for use. If this is insufficient for your use case, please contact us on Discord at #support.

There are several steps you need to perform to get a submission to work. 

<m-d-header text="Set up local system" h=3 add-hash></m-d-header>

First and foremost, set up your local environment so that Docker can access your GPU by following [these instructions](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker). Before continuing, you should be able to get the following output:

```
$ docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi

+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.142.00   Driver Version: 450.142.00   CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |
| N/A   23C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
```

<m-d-header text="Update starter kit" h=3 add-hash></m-d-header>

Next, we need to make some changes to the starter kit.

1. Make sure that the base Docker image you pick already contains the necessary Nvidia drivers and CUDA libraries. `pytorch/pytorch:1.9.0-cuda11.1-cudnn8-runtime` is an example of a good base image.
2. Edit the Docker platform configuration such that it has `run_args: [ "--gpus all" ]`.
3. Edit the Nextflow platform configuration such that it has the tag `gpu` attached to it.
4. Add a `gpu` tag to `scripts/nextflow.config`: `withLabel: gpu { maxForks = 1, containerOptions = '--gpus all' }`.

In the end, the platforms section of your Viash config should look something like this:
```yaml
platforms:
  - type: docker
    image: pytorch/pytorch:1.9.0-cuda11.1-cudnn8-runtime
    run_args: [ "--gpus all" ]
    setup:
      - type: python
        packages:
          - anndata 
  - type: nextflow
    labels: [ highmem, hightime, highcpu, gpu ]
```

The `scripts/nextflow.config` should contain the following:

```yaml
includeConfig "${launchDir}/target/nextflow/nextflow.config"

process {
  withLabel: lowcpu { cpus = 2 }
  withLabel: midcpu { cpus = 4 }
  withLabel: highcpu { cpus = 15 }
  withLabel: vhighcpu { cpus = 30 }
  withLabel: lowmem { memory = 10.GB }
  withLabel: midmem { memory = 20.GB }
  withLabel: highmem { memory = 55.GB }
  withLabel: vhighmem { memory = 110.GB }
  withLabel: lowtime { time = "10m" }
  withLabel: midtime { time = "20m" }
  withLabel: hightime { time = "30m" }

  withLabel: gpu {
    maxForks = 1
    containerOptions = '--gpus all'
  } // <- add this
}

def viash_temp = System.getenv("VIASH_TEMP") ?: "/tmp/"
docker.runOptions = "-v ${launchDir}/target/nextflow:${launchDir}/target/nextflow -v $viash_temp:$viash_temp --shm-size=4096m"
```

<m-d-header text="How to generate a submission from WSL2" h=2 add-hash></m-d-header>


1. Install WSL2: [https://docs.microsoft.com/en-us/windows/wsl/install](https://docs.microsoft.com/en-us/windows/wsl/install)
2. Install Docker Desktop with a WSL2 backend: [https://docs.docker.com/desktop/windows/wsl/](https://docs.docker.com/desktop/windows/wsl/)
3. Open Ubuntu from the Start Menu and run the following commands:
```bash
# update packages
sudo apt-get update
sudo apt-get upgrade -y

# test to see if docker works
docker run hello-world

# install dependencies
sudo apt-get install -y default-jdk unzip zip

# get starter kit
mkdir openproblems-neurips && cd openproblems-neurips
wget https://github.com/openproblems-bio/neurips2021_multimodal_viash/releases/latest/download/starter_kit-predict_modality-python.zip
unzip starter_kit-predict_modality-python.zip

# run everything to see if it works
scripts/0_sys_checks.sh
scripts/1_unit_test.sh
scripts/2_generate_submission.sh
scripts/3_evaluate_submission.sh
```
<m-d-header text="Can I generate a submission on Saturn Cloud?" h=2 add-hash></m-d-header>

Yes. Please follow [these instructions](/neurips_docs/submission/quickstart_saturncloud) to get started.

<m-d-header text="Quickstart SaturnCloud" h=1 add-hash></m-d-header>

The goal of this guide is to get you started developing submissions for the competition as quickly as possible. This section will detail the necessary steps to create your first submission.

<m-d-header text="1. Register on EvalAI" h=2 add-hash></m-d-header>

EvalAI is an open source platform to host machine learning competitions. We're using this tool to evaluate submissions and host the leaderboard. To register for the competition, you must:

1. [Create](https://eval.ai/auth/signup) an Account on EvalAI
2. [Visit](https://eval.ai/web/challenges/challenge-page/1111/overview) the Single-Cell Multimodal Data Integration Challenge page
3. [Register](https://eval.ai/web/challenges/challenge-page/1111/participate) by creating a new team on EvalAI. You can change your team details later or merge teams.

<m-d-header text="2. Register on Saturn Cloud" h=2 add-hash></m-d-header>

1. Go to https://saturncloud.io/ and login or create a free account
2. Under "Create a Resource" at the top of the page, click on the NeurIPS Openbio template card and click through the pop-up modals to create a new Saturn Cloud project using this template.
3. Click on the ▶ button to start a Jupyter server
4. Once the server is started, click the "Jupyter Lab" button to access the server
5. From the Saturn Cloud dashboard, you should see a blue circle in the lower right corner with a which chat icon inside. Click this button and send a message saying "Can you please upgrade my account for the Open Problems NeurIPS competition?". Someone at Saturn Cloud should respond shortly and you should see 100 hours appear under the "Hours remaining" box on the left sidebar.

<m-d-header text="3. Configure your local environment" h=2 add-hash></m-d-header>

First you need to install some requisites:
```
sudo apt-get update
sudo apt-get install -y unzip zip
```

In order to easily submit your solution to eval.ai, you should install and configure the evalai cli. Click [here](https://eval.ai/web/challenges/challenge-page/1111/submission) for more instructions. Please run:
```bash
pip install evalai
evalai set_token <your token here>
```
<m-d-header text="4. Grab a starter kit" h=2 add-hash></m-d-header>

You can find a set of starter kits for each task below or on the [GitHub releases](https://github.com/openproblems-bio/neurips2021_multimodal_viash/releases) of the competition codebase. Download the starter kit which is most relevant to you and unzip it in a directory.

Task 1 - Predict Modality

* [Starter Kit in Python](https://github.com/openproblems-bio/neurips2021_multimodal_viash/releases/latest/download/starter_kit-predict_modality-python.zip)
* [Starter Kit in R](https://github.com/openproblems-bio/neurips2021_multimodal_viash/releases/latest/download/starter_kit-predict_modality-r.zip)

Task 2 - Match Modality

* [Starter Kit in Python](https://github.com/openproblems-bio/neurips2021_multimodal_viash/releases/latest/download/starter_kit-match_modality-python.zip)
* [Starter Kit in R](https://github.com/openproblems-bio/neurips2021_multimodal_viash/releases/latest/download/starter_kit-match_modality-r.zip)

Task 3 - Joint Embedding

* [Starter Kit in Python](https://github.com/openproblems-bio/neurips2021_multimodal_viash/releases/latest/download/starter_kit-joint_embedding-python.zip)
* [Starter Kit in R](https://github.com/openproblems-bio/neurips2021_multimodal_viash/releases/latest/download/starter_kit-joint_embedding-r.zip)

Once you've chosen your starter kit, download and unzip it:

```bash
mkdir starter_kit
cd starter_kit
wget https://github.com/openproblems-bio/neurips2021_multimodal_viash/releases/latest/download/starter_kit-predict_modality-python.zip
unzip starter_kit-predict_modality-python.zip
ls -l # view contents
```
<m-d-header text="5. Tweak starter kit" h=2 add-hash></m-d-header>

Running Docker containers on Saturn Cloud is not enabled as it would pose security problems for other containers running on the same platform. This means we need to make some minor tweaks to the starter kit. Please run:

```bash
echo "" >> scripts/nextflow.config
echo "docker.enabled = false" >> scripts/nextflow.config
sed -i 's#docker info#echo hi#g' scripts/0_sys_checks.sh
sed -i 's#-p docker#-p native#g' scripts/1_unit_test.sh
sed -i 's#-p docker#-p native#g' scripts/2_generate_submission.sh
echo 'echo No local evaluation is possible on Saturn Cloud' > scripts/3_evaluate_submission.sh
```
<m-d-header text="6. Generate your first submission" h=2 add-hash></m-d-header>


To make sure your local environment is set up correctly, first run the `2_generate_submission.sh` script. This script triggers a Nextflow workflow to:
1. Build a [viash component](https://viash.io/docs/getting_started/what_is_a_viash_component/) of the method in `script.py/R` using the configuration specified in `config.vsh.yaml`.
2. Sync the training datasets from S3 (`s3://openproblems-bio/public/phase1-data`) to a local drive (`output/datasets/`).
3. Apply the containerized method on the training datasets.
4. Create a `submission.zip` file that can be uploaded to EvalAI for evaluation on the competition data and registration of the method on the leaderboard.

Please run the following commands:

```bash
scripts/0_sys_checks.sh
scripts/1_unit_test.sh
scripts/2_generate_submission.sh
```

If this workflow finishes successfully, you'll see something like this:
```
$ scripts/2_generate_submission.sh
...
######################################################################
##            Generating submission files using nextflow            ##
######################################################################
N E X T F L O W  ~  version 21.04.1
Pulling openproblems-bio/neurips2021_multimodal_viash ...
Launching `openproblems-bio/neurips2021_multimodal_viash` [intergalactic_roentgen] - revision: a784bb6c4b [main_build]
[f6/c705c5] process > method:method_process (openproblems_bmmc_multiome_phase1) [100%] 2 of 2 ✔

######################################################################
##                        Submission summary                        ##
######################################################################
Please upload your submission at the link below:
  https://eval.ai/web/challenges/challenge-page/1111/submission

Or use the command below create a private submission:
> evalai challenge 1111 phase 2278 submit --file submission.zip --large --private

Or this command to create a public one:
> evalai challenge 1111 phase 2278 submit --file submission.zip --large --public

Good luck!
```

Make note of the outputs generated in the nextflow step. If all went well, you should see a 100% success rate.

<m-d-header text="7. Submitting to EvalAI" h=2 add-hash></m-d-header>

To upload the submission, you have two options:
* Upload the submission via https://eval.ai/web/challenges/challenge-page/1111/submission
* Use the [EvalAI-CLI](https://github.com/Cloud-CV/evalai-cli) to upload the submission following the instructions outputted by the generate script.

Once your method is submitted, you can navigate to the [My Submissions](https://eval.ai/web/challenges/challenge-page/1111/my-submission) tab for the competition and select the phase that matches your recent submission. Here you will find a table that lists each submission you've uploaded for a given phase. Once the "Status" column is marked "Finished" you can view the `results.json` file that provides the method performance. Note that it might take up to 5 minutes for your submission to update from "Running" to "Finished", and that your submission might have to wait in a queue for an undetermined amount of time depending on the number of submissions being run on the submission server.

<m-d-header text="Encountering issues?" h=2 add-hash></m-d-header>

If you encounter a problem, please read the [FAQ](/neurips_docs/submission/faq/) to see whether a solution is already described. If you can't find a solution to your problem, please reach out on the competition [Discord](https://discord.gg/Q3RKGMGD3E) server. See the `#troubleshooting` or `#viash-help` channels.

<m-d-header text="Quickstart EvalAI" h=1 add-hash></m-d-header>


The goal of this guide is to get you started developing submissions for the competition as quickly as possible. This section will detail the necessary steps to create your first submission.

<m-d-header text="1. Register on EvalAI" h=2 add-hash></m-d-header>


EvalAI is an open source platform to host machine learning competitions. We're using this tool to evaluate submissions and host the leaderboard. To register for the competition, you must:

1. [Create](https://eval.ai/auth/signup) an Account on EvalAI
2. [Visit](https://eval.ai/web/challenges/challenge-page/1111/overview) the Single-Cell Multimodal Data Integration Challenge page
3. [Register](https://eval.ai/web/challenges/challenge-page/1111/participate) by creating a new team on EvalAI. You can change your team details later or merge teams.

<m-d-header text="2. Configure your local environment" h=2 add-hash></m-d-header>


For this competition, we want competitors to share code and we will evaluate results on a remote server. To facilitate running arbitrary scripts submitted by contestants, we are using [Viash](https://viash.io), a tool designed to turn R and Python scripts into Dockerized components that can be executed from the command line and integrated into a workflow system like [Nextflow](https://www.nextflow.io/).

Before you get started with the competition you will need to install two prerequisites:

1. Install [Docker](https://docs.docker.com/get-docker/)
2. Install Java Runtime ≥8 ≤12, available from [OpenJDK](https://adoptopenjdk.net/?variant=openjdk11&jvmVariant=hotspot)

{{< callout note >}}
**Tip:** Under Windows, first [install WSL2](https://openproblems.bio/neurips_docs/submission/faq/#how-to-generate-a-submission-from-wsl2).
{{</ callout >}}

<m-d-header text="3. Grab a starter kit" h=2 add-hash></m-d-header>
You can find a set of starter kits for each task below or on the [GitHub releases](https://github.com/openproblems-bio/neurips2021_multimodal_viash/releases) of the competition codebase. Download the starter kit which is most relevant to you and unzip it in a directory.

Task 1 - Predict Modality

* [Starter Kit in Python](https://github.com/openproblems-bio/neurips2021_multimodal_viash/releases/latest/download/starter_kit-predict_modality-python.zip)
* [Starter Kit in R](https://github.com/openproblems-bio/neurips2021_multimodal_viash/releases/latest/download/starter_kit-predict_modality-r.zip)

Task 2 - Match Modality

* [Starter Kit in Python](https://github.com/openproblems-bio/neurips2021_multimodal_viash/releases/latest/download/starter_kit-match_modality-python.zip)
* [Starter Kit in R](https://github.com/openproblems-bio/neurips2021_multimodal_viash/releases/latest/download/starter_kit-match_modality-r.zip)

Task 3 - Joint Embedding

* [Starter Kit in Python](https://github.com/openproblems-bio/neurips2021_multimodal_viash/releases/latest/download/starter_kit-joint_embedding-python.zip)
* [Starter Kit in R](https://github.com/openproblems-bio/neurips2021_multimodal_viash/releases/latest/download/starter_kit-joint_embedding-r.zip)


<m-d-header text="4. Generate your first submission" h=2 add-hash></m-d-header>

To make sure your local environment is set up correctly, first run the `2_generate_submission.sh` script. This script triggers a Nextflow workflow to:
1. Build a [viash component](https://viash.io/docs/getting_started/what_is_a_viash_component/) of the method in `script.py/R` using the configuration specified in `config.vsh.yaml`.
2. Sync the training datasets from S3 (`s3://openproblems-bio/public/phase1-data`) to a local drive (`output/datasets/`).
3. Apply the containerized method on the training datasets.
4. Create a `submission.zip` file that can be uploaded to EvalAI for evaluation on the competition data and registration of the method on the leaderboard.

If this workflow finishes successfully, you'll see something like this:

```
$ scripts/2_generate_submission.sh
...
######################################################################
##            Generating submission files using nextflow            ##
######################################################################
N E X T F L O W  ~  version 21.04.1
Pulling openproblems-bio/neurips2021_multimodal_viash ...
Launching `openproblems-bio/neurips2021_multimodal_viash` [intergalactic_roentgen] - revision: a784bb6c4b [main_build]
[f6/c705c5] process > method:method_process (openproblems_bmmc_multiome_phase1) [100%] 2 of 2 ✔

######################################################################
##                        Submission summary                        ##
######################################################################
Please upload your submission at the link below:
  https://eval.ai/web/challenges/challenge-page/1111/submission

Or use the command below create a private submission:
> evalai challenge 1111 phase 2278 submit --file submission.zip --large --private

Or this command to create a public one:
> evalai challenge 1111 phase 2278 submit --file submission.zip --large --public

Good luck!
```

Make note of the outputs generated in the nextflow step. If all went well, you should see a 100% success rate.

<m-d-header text="5. Submitting to EvalAI" h=2 add-hash></m-d-header>

To upload the submission, you have two options:
* Upload the submission via https://eval.ai/web/challenges/challenge-page/1111/submission
* Use the [EvalAI-CLI](https://github.com/Cloud-CV/evalai-cli) to upload the submission following the instructions outputted by the generate script.

Once your method is submitted, you can navigate to the [My Submissions](https://eval.ai/web/challenges/challenge-page/1111/my-submission) tab for the competition and select the phase that matches your recent submission. Here you will find a table that lists each submission you've uploaded for a given phase. Once the "Status" column is marked "Finished" you can view the `results.json` file that provides the method performance. Note that it might take up to 5 minutes for your submission to update from "Running" to "Finished", and that your submission might have to wait in a queue for an undetermined amount of time depending on the number of submissions being run on the submission server.

<m-d-header text="Encountering issues?" h=2 add-hash></m-d-header>

If you encounter a problem, please read the [FAQ](/neurips_docs/submission/faq/) to see whether a solution is already described. If you can't find a solution to your problem, please reach out on the competition [Discord](https://discord.gg/Q3RKGMGD3E) server. See the `#troubleshooting` or `#viash-help` channels.

<m-d-header text="Start Kit Contents" h=1 add-hash></m-d-header>

The Starter Kits have all you need to compete in the Multimodal Single-Cell Data Integration Challenge. The following sections will use the Python starter kit for the Modality Prediction task. After unzipping the starter kit, the working directory will contain the following files.

```
├── LICENSE                                 # MIT License
├── README.md                               # Some starter information
├── bin/                                    # Binaries needed to generate a submission
│   ├── check_format
│   ├── nextflow
│   └── viash
├── config.vsh.yaml                         # Viash configuration file
├── script.py                               # Script containing your method
├── sample_data/                            # Small sample datasets for unit testing and debugging
│   ├── openproblems_bmmc_cite_starter/     # Contains H5AD files for CITE data
│   └── openproblems_bmmc_multiome_starter/ # Contains H5AD files for multiome data
├── scripts/                                # Scripts to test, generate, and evaluate a submission
│   ├── 0_sys_checks.sh                     # Checks that necessary software installed
│   ├── 1_unit_test.sh                      # Runs the unit tests in test.py
│   ├── 2_generate_submission.sh            # Generates a submission pkg by running your method on validation data
│   ├── 3_evaluate_submission.sh            # (Optional) Scores your method locally
│   └── nextflow.config                     # Configurations for running Nextflow locally
└── test.py                                 # Default unit tests. Feel free to add more tests, but don't remove any.
```


The most important parts of this directory are the `script.py` and `config.vsh.yaml` files, which you will need to modify in order to submit your method to EvalAI.

<m-d-header text="Anatomy of script.py" h=2 add-hash></m-d-header>

The Python script provided in this starter kit has three main sections: Imports, Viash block, and Method.

<m-d-header text="Imports" h=3 add-hash></m-d-header>

This section defines which packages the method expects, if you want to import a new different package, add the `import` statement here **and** add the dependency to `config.vsh.yaml` (See below). This ensures that we can build an appropriate image to run your code.

```python
import logging
import anndata as ad
import numpy as np

logging.basicConfig(level=logging.INFO)
```

<m-d-header text="Viash block" h=3 add-hash></m-d-header>

This optional code block exists to facilitate prototyping so your script can run when called directly by running `python script.py` (or `Rscript script.R` for R users). 

```python
## VIASH START
# Anything within this block will be removed by `viash` and will be
# replaced with the parameters as specified in your config.vsh.yaml.
input_path = 'sample_data/openproblems_bmmc_multiome_starter/openproblems_bmmc_multiome_starter'
par = {
    # Required arguments for the task
    'input_train_mod1': input_path + '.train_mod1.h5ad',
    'input_train_mod2': input_path + '.train_mod2.h5ad',
    'input_test_mod1': input_path + '.test_mod1.h5ad',
    'output': 'output.h5ad',
    # Optional method-specific arguments
    'n_neighbors': 5,
}
meta = { 'resources_dir': '.', 'functionality_name': '.' }
## VIASH END
```

Here, the `par` dictionary contains all the `arguments` defined in the `config.vsh.yaml` file. When you create the component using the `generate_submission.sh` script, Viash will remove everything between the `## VIASH START` and `## VIASH END` comments and replace it with a dictionary / named list containing the parameter values retrieved from an auto-generated command-line interface.

<m-d-header text="Method" h=3 add-hash></m-d-header>

This code block will typically consist of reading the input files, performing some preprocessing, training a model on the train cells, generating predictions for the test cells, and outputting the predictions as an AnnData file (See [Data Formats](/neurips_docs/submission/data_formats)).

```python
## Data reader
logging.info('Reading `h5ad` files...')

input_train_mod1 = ad.read_h5ad(par['input_train_mod1'])
input_train_mod2 = ad.read_h5ad(par['input_train_mod2'])
input_test_mod1 = ad.read_h5ad(par['input_test_mod1'])

# ... preprocessing ... 
# ... train model ...
# ... generate predictions ...

# write output to file
adata = ad.AnnData(
    X=y_pred,
    uns={
        'dataset_id': input_train_mod1.uns['dataset_id'],
        'method_id': "python_starter_kit", # all lowercase letters or underscores
    },
)
adata.write_h5ad(par['output'], compress='gzip')
```

<m-d-header text="Anatomy of config.vsh.yaml" h=2 add-hash></m-d-header>

Once the script is working, you'll probably need to update the `config.vsh.yaml` file. The configuration has two sections:
* Functionality about the script, including method arguments and any additional resources (e.g. pretrained models)
* Platform targets that define the components dependencies (which are used to build a Docker container).

Full documentation on the Viash configuration file is available on the [Viash documentation site](https://viash.io/docs/reference_config/config/).

<m-d-header text="Functionality" h=3 add-hash></m-d-header>

The first section of the configuration file contains information about the metadata of the script including parameters for the script and a list of resource files. 

```yaml
functionality:
  # a unique name for your method, same as what is being output by the script.
  # must match the regex [a-z][a-z0-9_]*
  name: method

  # metadata for your method
  description: A description for your method.
  authors:
    - name: John Doe
      email: johndoe@youremailprovider.com
      roles: [ author, maintainer ]
      props: { github: johndoe, orcid: "0000-1111-2222-3333" }

  # component parameters
  arguments:
    # required inputs (do not modify)
    - name: "--input_train_mod1"
      type: "file"
      example: "dataset_mod1.h5ad"
      description: Censored dataset, training cells.
      required: true
    - name: "--input_test_mod1"
      type: "file"
      example: "dataset_mod1.h5ad"
      description: Censored dataset, test cells.
      required: true
    - name: "--input_train_mod2"
      type: "file"
      example: "dataset_mod2.h5ad"
      description: Censored dataset.
      required: true
    # required outputs (do not modify)
    - name: "--output"
      type: "file"
      direction: "output"
      example: "output.h5ad"
      description: Dataset with predicted values for modality2.
      required: true
    # Method-specific parameters. 
    # Change these to expose parameters of your method to Nextflow (optional)
    - name: "--n_neighbors"
      type: "integer"
      default: 5
      description: Number of neighbors to use.

  # files your script needs
  resources:
    # the script itself
    - type: python_script
      path: script.py
    # additional resources your script needs (optional)
    - path: pretrained_model.pt
```

In this section of the configuration you should focus on updating the following sections:
1. Description and Authors - Information about who contributed the method
2. Arguments - Each section here defines a command-line argument for the script. These sections are all passed to the script in the form of a dictionary called `par`. You only need to change the method-specific parameters, and if you would like these parameters hard-coded into the script, you do not need to provide any parameters here. Make sure **not** to remove required parameters such as the inputs. Your method cannot be executed properly otherwise!
3. Resources - This section describes the files that need to be included in your component. For example if you'd like to add a file containing model weights called `weights.pt`, add `{ type: file, path: weights.pt }` to the resources. You can now load the additional resource in your script by using at the path `meta['resources_dir'] + '/weights.pt'`.


<m-d-header text="Platform" h=3 add-hash></m-d-header>

The Platform section defines the information about how the Viash component is run on various backend platforms. The two platforms that are important for the competition are:

1. Docker ([docs](https://viash.io/docs/reference_config/platform-docker/))
2. Nextflow ([docs](https://viash.io/docs/reference_config/platform-nextflow/))

```yaml
# target platforms
platforms:

  # By specifying 'docker' platform, viash will build a standalone
  # executable which uses docker in the back end to run your method.
  - type: docker
    # you need to specify a base image that contains at least bash and python
    image: dataintuitive/randpy:py3.8
    # You can specify additional dependencies with 'setup'.
    # See https://viash.io/docs/reference_config/platform-docker/#setup-list
    # for more information on how to add more dependencies.
    setup:
      # - type: apt
      #   packages:
      #     - bash
      # - type: python
      #   packages:
      #     - scanpy
      - type: python
        packages:
          - scikit-learn
          - anndata
          - scanpy

  # By specifying a 'nextflow', viash will also build a viash module
  # which uses the docker container built above to also be able to
  # run your method as part of a nextflow pipeline.
  - type: nextflow
    labels: [ lowmem, lowtime, lowcpu ]
```

The most important part of this section to update is the `setup` definition that describes the packages that need to be installed in the docker container for the method to run. There are many different methods for specifying these requirements described in the Viash [docs](https://viash.io/docs/reference_config/platform-docker/#setup-list).

You can also change the memory, runtime, and CPU utilization be editing the Nextflow labels section. Available options are `[low|med|high]` for each of `mem`, `time`, and `cpu`. The corresponding resource values can be found in the `scripts/nextflow.config` file.

<info-block>
**Tip:** After making changes to the components dependencies, you will need to rebuild the docker container as follows:

```sh
$ bin/viash run -- ---setup cachedbuild
[notice] Running 'docker build -t method:dev /tmp/viashsetupdocker-method-tEX78c'
```
</info-block>

<info-block>
**Tip #2:** You can view the dockerfile that Viash generates from the config file using the `---dockerfile` argument:
```sh
$ bin/viash run -- ---dockerfile
```
</info-block>



<m-d-header text="Input/output AnnData file format" h=2 add-hash></m-d-header>

All the datasets provided in the competition are organized in [AnnData](https://anndata.readthedocs.io) objects. This page describes the structure of AnnData objects so that you can work with the data in the competition efficiently.

AnnData is a generic data storage format especially well-suited for single-cell data. The class stores a data matrix `X` together with annotations of observations and variables in Pandas DataFrames. The advantage of storing the annotations and data in the same object is to ensure that when inspecting slices of the data, relevant annotations are properly linked with the views of the data.

The most common attributes are:
* `adata.X` - The data. This can by a sparse array or a numpy array.
* `adata.n_obs` - The number of observations, equal to `adata.X.shape[0]`
* `adata.obs_names` - The index for the observations, similar to a Pandas DataFrame `index`. Typically this contains cell barcodes.
* `adata.obs` - A Pandas DataFrame containing annotations of the observations, for example, cluster labels or the donor of origin. The index of the DataFrame must be the same as `adata.obs_names`.
* `adata.obsm` - A dictionary of n-dimensional arrays (ndarrays) of shape `(n_obs, m)`. For example, it is common to store a PCA or UMAP embedding of the data in `adata.obsm["X_pca"]` or `adata.obsm["X_umap"]`.
* `adata.obsp` - Pairwise annotations of the observations. Most often this contains a graph adjacency matrix where the nodes in the graph are observations.
* `adata.n_var` - The number of variables or features in the data, equal to `adata.X.shape[1]`
* `adata.var_names` - The index for the variables, similar to a Pandas DataFrame `columns`. Typically this contains gene names.
* `adata.var` - Annotations on the variables, e.g. whether a feature is a protein coding gene or a non-coding gene. Index of this DataFrame must match `adat.var_names`.
* `adata.varm`, `adata.varp` - Analogous to the `obsm` and `obsp` attributes. Not commonly used.
* `adata.uns` - A dictionary to story any unstructured annotations associated with the data. E.g, a data version.

The following diagram shows these attributes and their relation to the data:

<figure>
  <img style="width:100%;max-width:800px;" src="/media/learning/anndata.svg">
  <figcaption>
    <h3>
      AnnData  - Annotated Data Containers
    </h3>
    <p style="font-size: medium;">
      AnnData provides a scalable way of keeping track of data and learned annotations.
    </p>
  </figcaption>
</figure>

AnnData objects can be opened in Python using the [`anndata.read_h5ad()`](https://anndata.readthedocs.io/en/latest/anndata.read_h5ad.html) function or the [`scanpy.read_h5ad()`](https://scanpy.readthedocs.io/en/stable/generated/scanpy.read_h5ad.html) function. This object can be opened in R using the [`anndata::read_h5ad()`](https://anndata.dynverse.org/index.html#getting-started) function.

For the full AnnData API, please consult the [documentation of the `anndata` package](https://anndata.readthedocs.io/en/latest/anndata.AnnData.html).

<m-d-header text="Terms" h=1 add-hash></m-d-header>
<m-d-header text="Terms and Conditions" h=2 add-hash></m-d-header>

Welcome to the NeurIPS 2021 Multimodal Single-Cell Data Integration challenge (“Challenge”). These are the Terms and Conditions (“Terms”) of the Challenge. The Challenge is organized by the individuals listed in the organizers of the competition webpage at https://openproblems.bio/neurips_2021/#organizers ("the Organizers"). Cellarity, Yale, Chan Zuckerberg Biohub, and Helmholtz Munich shall provide any data sets solely as a sponsor of the Challenge.

Please read the Terms carefully. By agreeing with the Terms you acknowledge that you have read and understood the Terms, have had the opportunity to review the Terms with legal counsel even if you have chosen not to do so, accept these Terms and agree to be bound by them. If you don’t agree with (or cannot comply with) the Terms, then you may not participate in this Challenge.

By signing up, downloading any Challenge Data Sets or otherwise participating in any manner in the Challenge, you are entering into a binding contract with the Organizers. These Terms apply to that contact.

<m-d-header text="Eligibility" h=2 add-hash></m-d-header>

In order to access the Challenge Data Set and participate in this Challenge, you need to be 18 years or older or the age of majority in the jurisdiction of your residence.

You represent and warrant that you are free to enter into this contract with the Organizers, and you are not bound by any conflicting obligations or conditions whether resulting from employment or student relationship or otherwise. The Organizers do not require your full name or any other disclosure of your identity by the time you sign up for the Challenge. No purchase is necessary to participate or win a Prize (defined below) in this Challenge.

Local rules and regulations may apply to each individual participant, so please check your local laws to ensure that you are eligible to participate in this Challenge.

The Organizers of the Challenge may in their sole discretion disqualify any participant they reasonably believe has attempted to undermine the legitimate operation of the Challenge by cheating, deception, other unfair practices or abuses, threatens or harasses any other participants, the Organizers or Sponsors.

<m-d-header text="Participation" h=2 add-hash></m-d-header>

This Challenge is a skills-based competition to promote and further the field of single-cell data science. In order to participate, you must follow all instructions and requirements posted on the Website page titled ["Development process"](/neurips_docs/submission/development_process/).

You can participate in this Challenge alone or as a team. There is no maximum team size, and the minimum team size is one person. Each individual participant needs to create an EvalAI account as described on the Website. Participants may not collaborate unless those participants entered this Challenge as a team; participants who are members of the same team may collaborate with one another. Teams may be merged if the merging teams notify the Organizers accordingly. Teams may be merged only during the Validation phase. After the Validation phase ends and the Testing phase starts teams merging isn’t allowed anymore. If you participate in this Challenge as part of a team, then you may participate in one team only.

The Challenge will run from the start date and time to the end date and time set forth on the Website and may be subject to change. Any updated or additional deadlines will be publicized on the Website. It is your responsibility to check the Website regularly to stay informed of any deadline changes. YOU ARE RESPONSIBLE FOR DETERMINING THE CORRESPONDING TIME ZONE IN YOUR LOCATION.

<m-d-header text="Submission" h=2 add-hash></m-d-header>

Conditioned upon compliance with these Terms, the Organizers will award Prizes to the participants with the highest scores, as evaluated by the Organizers according to the Requirements, in its sole discretion. The Prize will be sponsored by Cellarity.

In order to be considered for a Prize, your Challenge submission (“Submission”) must conform to, and be submitted in the manner, form, and timeline described in the Requirements set forth on the Website. Your Submission must include the source code and any related information used to derive the results contained in your Submission. The source code must be released under a permissive open-source license (MIT License is preferred, BSD or Apache are acceptable). A third party should be able to use your submitted source to regenerate your results. Please read the Website carefully and contact the Organizers with any questions. The Organizers will score your Submission as described on the Website. Organizers or Twitter may contact participants and ask to publish any source code. In keeping with the goals of the NeurIPS 2020 Black-Box Optimization Challenge, you agree that by entering a Submission, you represent and warrant that to the best of your knowledge, the Submission does not infringe the intellectual property rights (including copyrights, patents, trademarks or trade secrets) of any third party. You agree that no consideration will be due by the Organizers, the Sponsor or any other party in connection with any such intellectual property rights, other than as set forth in these Terms.
Submissions are void if they are in whole or in part illegible, incomplete, damaged, altered, counterfeit, obtained through fraud or, late. Organizers and Sponsor reserve the right, in their sole discretion, to disqualify any participant who makes a Submission that does not meet the Requirements or is in violation of these Terms.

<m-d-header text="Determining Winners" h=2 add-hash></m-d-header>

Each Submission will be scored and ranked by the evaluation metrics set forth on the Website. The top-scoring Submissions will be selected as the potential winner(s) of the Challenge. Final results are determined solely by the leaderboard ranking on the private leaderboard as set forth on the Website, subject to compliance with these Terms.

In the event of a tie, the Submission that was entered first to the Challenge will be the winner. In the event a potential winner is disqualified for any reason, the Submission that received the next highest score rank will be chosen as the potential winner.

The Organizers will notify the potential winner(s) by email. If a potential winner does not respond to the notification attempt within five (5) days from the first notification attempt, then such potential winner will be disqualified and an alternate potential winner will be selected from among all eligible entries received based on the judging criteria described herein.

The Organizers' determination of the winners is final and binding. Cellarity is only a sponsor of the Challenge and is not responsible for determining the winners of the Challenge.

<m-d-header text="Winner’s Obligations" h=2 add-hash></m-d-header>

As a condition to being awarded a Prize, a Prize winner must publish the final model’s software code as used to generate the winning Submission and associated documentation and paper in the manner and form described on the Website and share with Organizers the location where the materials are published; the delivered software code must be capable of generating the winning Submission and contain a description of resources required to build and/or run the executable code successfully.

<m-d-header text="Prizes" h=2 add-hash></m-d-header>

The prize(s) for this Challenge (“Prize(s)”) are described on the Website. Odds of winning any prize depend on the number of eligible Submissions received by the Organizers and the skill of the participants.
All Prizes are subject to the Organizers’ Review and verification of the participant’s eligibility and compliance with these Terms, and the compliance of the winning Submissions with the Requirements. In the event that the Submission demonstrates non-compliance with these Terms or the Requirements, Organizers may at their sole discretion take either of the following actions: (a) disqualify the Submission(s); or, (b) require the potential winner(s) to remediate within one week after notice of all issues identified in the Submission(s).

A potential winner may decline to be nominated as a Challenge winner by notifying the Organizers directly within one week after the end date and time of the Challenge as noted on the Website, in which case the potential winner forgoes any prize or other features associated with winning the Challenge.
Potential winners (including each team member, if applicable) must return all required prize acceptance documents within 15 days following notification, or such potential winner will be deemed to have forfeited the prize and another potential winner will be selected. Prize(s) will be awarded within approximately 30 days after receipt by the Organizers of the required Prize acceptance documents.
You are not eligible to receive any Prize if you do not meet the Eligibility requirements in Section 2 above.
If a team wins a monetary Prize, the Prize money will be allocated in even shares between the eligible team members.

Any valuation of the Prize(s) is based on available information provided to the Organizers and the value of any prize awarded to a winner must be reported for tax purposes as required by law. Each winner is solely responsible for reporting and paying any and all applicable taxes related to the Prize(s) and paying any expenses associated with any Prize.

<m-d-header text="Publicity" h=2 add-hash></m-d-header>
By accepting a Prize, you agree that the Organizers and its agencies may use your name and/or likeness for advertising and promotional purposes without additional compensation, unless prohibited by law.

<m-d-header text="Privacy" h=2 add-hash></m-d-header>
The organizers take your privacy very seriously. You understand that in order to administer the Challenge and award a Prize, you will be required to provide personal data to Organizers and Cellarity, such as your name and email address. You consent to Organizers and Cellarity's collection and processing (including sharing and storage) of your personal data in relation to this Challenge in compliance with the applicable data protection laws. If you have any questions about how we process your personal data, or if you want us to correct, block or delete your personal data, please contact the Organizers through the contact form on the Website.

<m-d-header text="Warranty" h=2 add-hash></m-d-header>
You warrant that your Submission is your own original work and, as such, you are the sole and exclusive owner and rights holder of the Submission, and you have the right to make the Submission and grant all required licenses. You agree not to make any Submission that infringes any third-party proprietary rights, intellectual property rights, industrial property rights, personal or moral rights or any other rights, including without limitation, copyright, trademark, patent, trade secret, privacy, publicity or confidentiality obligations; or otherwise violates any applicable law.

<m-d-header text="Indemnity" h=2 add-hash></m-d-header>
To the maximum extent permitted by law, you agree to indemnify and hold harmless the Organizers at all times from and against any liability, claims, demands, losses, damages, costs and expenses resulting from any act, default, or omission of the participant and/or a breach of any warranty set forth in these Terms. To the maximum extent permitted by law, you agree to defend, indemnify and hold the Organizers harmless from and against any and all third-party claims, actions, suits, or proceedings, as well as any and all losses, liabilities, damages, costs, and expenses (including reasonable attorney fees) arising out of or accruing from: (a) your Submission, or other material uploaded or otherwise provided by you that infringes any copyright, trademark, trade secret, trade dress, patent or other intellectual property rights of any third party, or defames any person or violates their rights of publicity or privacy; (b) any misrepresentation made by you in connection with the Challenge; (c) any non-compliance by you with these Terms; (d) claims brought by persons or entities other than the parties to these Terms arising from or related to your involvement with the Challenge; and, (e) your acceptance, possession, misuse or use of any Prize, or your participation in the Challenge and any Challenge-related activity.

Organizers and Twitter are not responsible for any malfunction of the Website or any late, lost, damaged, misdirected, incomplete, illegible, undeliverable, or destroyed Submissions or entry materials due to system errors; failed, incomplete, or garbled computer or other telecommunication transmission malfunctions; hardware or software failures of any kind; lost or unavailable network connections; typographical or system/human errors and failures; technical malfunction(s) of any telephone network or lines, cable connections, satellite transmissions, servers or providers, or computer equipment; traffic congestion on the Internet or at the Website; or, any combination thereof, which may limit a participant’s ability to participate. You hereby release Organizers and Twitter from any liability associated with any malfunction or other problem with the Website; any error in the collection, processing, or retention of any Submission; or any typographical or other error in the printing, offering or announcement of any Prize or winners.
Twitter is only a Sponsor of the Challenge, therefore You waive any and all claims of liability against Twitter for administration of the Challenge.

<m-d-header text="Right to Cancel, Modify, or Disqualify" h=2 add-hash></m-d-header>

If for any reason the Challenge cannot be run as planned due to reasons beyond the Organizers’ reasonable control, including but not limited to infection by computer virus, bugs, tampering, unauthorized intervention, fraud, technical failures, or any other causes which corrupt or affect the administration, security, fairness, integrity, or proper conduct of the Challenge, Organizers reserve the right at their sole discretion to cancel, terminate, modify, or suspend the Challenge. The Organizers further reserve the right to disqualify any participant who tampers with the submission process or any other part of the Challenge or Website. Any attempt by a participant to deliberately damage any website, including the Website, or undermine the legitimate operation of the Challenge is a violation of these Terms and should such an attempt be made, the Organizers reserve the right to seek damages from any such participant to the fullest extent of the applicable law.

<m-d-header text="No Offer of Employment" h=2 add-hash></m-d-header>

Under no circumstances shall the entry of a Submission, the awarding of a Prize, or anything in these Terms be construed as an offer or contract of employment with the Organizers or Cellarity. You acknowledge that you have submitted your Submission voluntarily and not in confidence or in trust. You acknowledge that no confidential, fiduciary, agency or other relationship, or implied-in-fact contract now exists between you, the Organizers and Cellarity and that no such relationship is established by the entry of your Submission.

<m-d-header text="Severability" h=2 add-hash></m-d-header>

The invalidity or unenforceability of any provision of these Terms shall not affect the validity or enforceability of any other provision hereof. If any provision is held invalid, illegal or unenforceable in any jurisdiction, then, to the fullest extent permitted by law, all other provisions hereof will remain in full force and effect.

<m-d-header text="Waiver" h=2 add-hash></m-d-header>

No delay or omission in exercising any right hereunder will operate as a waiver of that or any other right. A waiver or consent given on one occasion will not be construed as a bar to or waiver of any right on any other occasion. Any waiver must be in writing.

<m-d-header text="Integration" h=2 add-hash></m-d-header>

These Terms constitute the entire agreement between you, Organizers, and Cellarity concerning the Challenge and supersede any prior or contemporaneous agreements concerning the Challenge.



<m-d-header text="Multimodal Single-Cell Data Integration" h=1 add-hash></m-d-header>

A NeurIPS Competition (2021)

 <a href="https://docs.google.com/forms/d/e/1FAIpQLSe90Oky4-1b0HbdLsp5Yqo9juCd2mq-NlGHU9NHRW1ECok1xQ/viewform" target="blank"><button type="button" class="btn btn-primary" style="font-size: 1rem; height: 40px; padding: 0px 15px"><strong>Sign up for updates</strong></button></a>

<a href="https://openproblems.bio/neurips_docs/about/getting_started/" target="blank"><button type="button" class="btn btn-secondary" style="font-size: 1rem; height: 40px; padding: 0px 15px"><strong>Get Started</strong></button></a>

<a href="https://www.youtube.com/watch?v=ZXDILOyiy7A" target="blank"><button type="button" class="btn btn-secondary" style="font-size: 0.8rem; height: 25px; padding: 0px 15px"><strong>Watch the video</strong></button></a>
<a href="https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/158f3069a435b314a80bdcb024f8e422-Abstract-round2.html" target="blank"><button type="button" class="btn btn-secondary" style="font-size: 0.8rem; height: 25px; padding: 0px 15px"><strong>Read the paper</strong></button></a>
<a href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE194122" target="blank"><button type="button" class="btn btn-secondary" style="font-size: 0.8rem; height: 25px; padding: 0px 15px"><strong>Download the data</strong></button></a>

<m-d-header text="Details" h=1 add-hash></m-d-header>

The competition will focus on three tasks:
1. **Predicting one modality from another** - Given one modality, predict the other.
2. **Modality alignment** - Re-align multi-modal data measured in the same cells when we hide the correspondences between the two measurements.
3. **Learning a joint embedding** - Learn a meaningful embedding of multimodal data measured in the same cells that preserves expert-annotated data organization

To get started:

1.  Read [about the tasks](/neurips_docs/about_tasks) and the [submission quickstart](/neurips_docs/submission/quickstart/) on our competition website
2.  View the [starter kit contents](/neurips_docs/submission/starter_kit_contents)
4.  Explore the data and prototype methods for free on [Saturn Cloud](https://openproblems.bio/neurips_docs/about/explore) (Optional)
5.  Implement your method and [generate a submission](/neurips_docs/submission/development_process/)!

Full details about all the above can be found in the [Competition Documentation](/neurips_docs).

If you ever have any questions, please feel free to reach out on the Open Problems [Discord Server](https://discord.gg/hDE5bYEcHF).

If you'd like to get updates, please fill out the interest list below:

<a href="https://docs.google.com/forms/d/e/1FAIpQLSe90Oky4-1b0HbdLsp5Yqo9juCd2mq-NlGHU9NHRW1ECok1xQ/viewform" target="blank"><button type="button" class="btn btn-primary" style="font-size: 1rem; border: 1px solid #c4c4c4; height: 40px; padding: 0px 15px;"><strong>Sign up for updates</strong></button></a>

<m-d-header text="Organizers" h=1 add-hash></m-d-header>
<m-d-header text="Organizering Team" h=2 add-hash></m-d-header>


_In alphabetical order_

-   **Daniel Burkhardt** (<a href="mailto:dburkhardt@cellarity.com?subject=%5BNeurIPS_2021%5D" style="color: #EB5252;">Primary contact</a>) is a Machine Learning
    Scientist at [Cellarity.](https://cellarity.com) He is a core organizer of the Open Problems
    in Single-Cell Analysis. Daniel completed his PhD in Genetics at
    Yale University with a specialization in machine learning under the
    supervision of Smita Krishnaswamy.

-   **Jonathan M. Bloom** leads Perturbation Biology and Machine Learning at Cellarity. As an Institute Scientist at the Broad Institute, he cofounded the Hail team, Learning Meaningful Representations of Life, and Models, Inference & Algorithms, while contributing to sequencing benchmarks, statistical genetics, ML theory, and neuroscience efforts. Jonathan completed his PhD at Columbia and Moore Instructorship and NSF postdoc at MIT, conducting research on algebraic topology and geometry while reimagining the teaching of statistics.  

-   **Robrecht Cannoodt** is a postdoctoral researcher in Saeys lab at 
    VIB-UGent and a computer science consultant at Data Intuitive.
    During his PhD, his research was focused mainly on unsupervised learning in 
    single-cell omics, more specifically on trajectory inference. Robrecht 
    oversees infrastructure development for building collaborative, scalable 
    and reproducible pipelines using NextFlow and Viash.

-   **Smita Krishnaswamy** is an associate professor of Computer Science and
    Genetics at Yale University. Her research focuses on unsupervised
    learning, using data geometry, topology and deep learning methods
    for big biomedical data. She is a faculty advisor for Open Problems
    in Single-Cell Analysis.

-   **Malte Lücken** is a senior postdoctoral researcher and team leader
    for integrative single-cell analysis in the Machine Learning Group
    of Prof. Fabian Theis. His research focuses on evaluating
    computational methods for single-cell analysis and investigating how
    environmental stimuli and natural variation manifests on the level
    of single cells.

-   **Debora Marks** is an
    Associate Professor in the Department of Systems Biology at Harvard
    Medical School. Over the past five years, her lab has
    developed methods that accelerate structural biology with
    applications to cryoEM, crystallography, protein design and computed
    3D structures of thousands of proteins with unknown folds, protein
    complexes and RNA interactions, as well as flexible, dynamic and
    even disordered protein ensembles.

-   **Angela Pisco** is the Associate Director of Bioinformatics at the Chan Zuckerberg Biohub.
    Her group is responsible for generating and annotated atlas-scale single-cell
    datasets across biological systems.

-   **Bastian Rieck** is a senior assistant in the Machine Learning and
    Computational Biology Lab of Prof. Dr. Karsten Borgwardt at ETH
    Zurich. His main research interests are manifold learning approaches
    based on concepts from topological data analysis, with a focus on
    personalized medicine and computational biology.

-   **Jian Tang** is currently an assistant professor at HEC Montreal
    (business school of University of Montreal) and a core faculty
    member at Mila-Quebec AI Institute. His research focuses on graph
    representation learning, graph neural networks, drug discovery, and
    knowledge graphs.

-   **Fabian Theis** is the Professor of Mathematical Modeling of Biological
    Systems at the Department of Mathematics of the Technical University
    of Munich; the director of the Institute of Computational Biology,
    Helmholtz Zentrum München; the scientific director of the Helmholtz
    Artificial Intelligence Cooperation Unit; and a faculty member of
    the Wellcome Trust Sanger Institute, Cambridge, UK. His lab develops innovative
    methods for single-cell analysis.

-   **Alexander Tong** is a PhD candidate in Computer Science in the lab of
    Prof. Smita Krishnaswamy at Yale University. His research focuses on
    manifold learning and optimal transport with a focus on single-cell
    data. Alexander will be responsible for supporting the comparison
    infrastructure and evaluating submissions.

-   **Guy Wolf** is an assistant professor in the Department of Mathematics
    and Statistics at the Université de Montréal and a Core Member of
    Mila---the Québec AI Institute. His research
    focuses on manifold learning, representation learning, and geometric
    deep learning for exploratory data analysis, including methods for
    dimensionality reduction, visualization, denoising, data
    augmentation, and coarse graining, with particular focus on
    applications in biomedical data exploration.


<m-d-header text="Sponsers" h=1 add-hash></m-d-header>
<div class="row">
  <div class="col-md-1"></div>
  <div class="col-md-5">
    <div class="card" style="width: 300px; height:300px;">
      <img class="card-img-top" src="/media/sponsor/logotypemarkcolor.svg" alt="Card image cap" style="width:200px;  margin: 40px 50px 20px;">
      <div class="card-body" style="padding-top: 0px;">
        <p class="card-text">Cellarity is redefining drug discovery by targeting cell behaviors as opposed to individual proteins.<br><a class="stretched-link" href="https://cellarity.com/careers?utm_source=neurips&utm_medium=push-notification&utm_campaign=neurips&utm_content=neurips-competition">Learn more.</a></p>
      </div>
    </div>
  </div>


  <div class="col-md-5">
    <div class="card" style="width: 300px; height:300px;">
      <img class="card-img-top" src="/media/sponsor/CZI_Logotype.svg" style="width:150px; margin: 20px 60px 10px; ">
      <div class="card-body" style="padding-top: 0px;">
        <p class="card-text" style="line-height: 1.3rem;">CZI leverages technology and collaboration to accelerate progress in science, education and community work.<br> <a class="stretched-link" href="https://czi.org">Learn more.</a></p>
      </div>
    </div>
  </div>
</div>

<div class="row">
  <div class="col-md-4">
    <div class="card" style="width: 250px; height:120px;">
      <img class="card-img-top" src="/media/sponsor/biohub_logo.svg" alt="Card image cap" style="width:170px;  margin: 25px 40px;">
      <div class="card-body" style="padding-top: 0px;">
        <a class="stretched-link" href="https://www.czbiohub.org/"></a>
      </div>
    </div>
  </div>

  <div class="col-md-4">
    <div class="card" style="width: 250px; height:120px;">
      <img class="card-img-top" src="/media/sponsor/yale-logo-sprite.svg" alt="Card image cap" style="width:130px;  margin: 22px 60px;">
      <div class="card-body" style="padding-top: 0px;">
        <a class="stretched-link" href="https://www.yale.edu/"></a>
      </div>
    </div>
  </div>


  <div class="col-md-4">
    <div class="card" style="width: 250px; height:120px;">
      <img class="card-img-top" src="/media/sponsor/helmholtz_logotype.svg" alt="Card image cap" style="width:225px;  margin: 40px 12.5px">
      <div class="card-body" style="padding-top: 0px;">
        <a class="stretched-link" href="https://www.helmholtz-muenchen.de/icb/index.html"></a>
      </div>
    </div>
  </div>
</div>

<m-d-header text="Summary" h=1 add-hash></m-d-header>

Scaling from a dozen cells a decade ago to millions of cells today, single-cell measurement technologies are driving a revolution in the life sciences. Recent advances make it possible to measure multiple high-dimensional modalities (e.g. DNA accessibility, RNA, and proteins) simultaneously in the same cell. Such data provides, for the first time, a direct and comprehensive view into the layers of gene regulation that drive biological diversity and disease.

In this competition for [NeurIPS 2021,](https://neurips.cc/Conferences/2021/CompetitionTrack) we present three tasks on multimodal single-cell data using a first-of-its-kind multi-omics benchmarking dataset. Teams will predict one modality from another and learn representations of multiple modalities measured in the same cells. Progress will reveal how a common genetic blueprint gives rise to distinct cell types and processes, as a foundation for improving human health.

<v-youtube id="y5YbM0tbvCo"></v-youtube>

<br>
To learn more, you can watch a [lecture](https://www.youtube.com/watch?v=ZXDILOyiy7A) presented at the Broad Institute's Models Inferences and Algorithms meeting (<a href="https://drive.google.com/file/d/1olW-WN-kHYuG15MSgAK3GUqNtMCPFBq3/view?usp=sharing" style="color: #EB5252;">slides</a>).

<m-d-header text="Winners" h=1 add-hash></m-d-header>

We're proud to announce the winners of our 2021 NeurIPS competition!

<m-d-header text="Task 1 - Modality Prediction" h=2 add-hash></m-d-header>

_Given one modality, predict the other._

<m-d-header text="GEX→ATAC - Living Systems Lab" h=3 add-hash></m-d-header>

**KAUST, [code](https://github.com/openproblems-bio/neurips2021_multimodal_topmethods/tree/main/src/predict_modality/methods/LS_lab)**   
Aidyn Ubingazhibov, Sumeer Khan, Robert Lehman, Xabier Martinez De Morentin, Minxing Pang, David Gomez-Cabrero, Narsis Kiani, Jesper Tegner

<m-d-header text="ATAC→GEX - Cajal" h=3 add-hash></m-d-header>

**Francis Crick Institute, [code](https://github.com/openproblems-bio/neurips2021_multimodal_topmethods/tree/main/src/predict_modality/methods/cajal)**  
Anna Laddach, Roman Laddach, Michael Shapiro


<m-d-header text="GEX→ADT - Dengkw" h=3 add-hash></m-d-header>

**University of Michigan, [code](https://github.com/openproblems-bio/neurips2021_multimodal_topmethods/tree/main/src/predict_modality/methods/Guanlab-dengkw)**  
Kaiwen Deng

<m-d-header text="ADT→GEX - Novel" h=3 add-hash></m-d-header>

**Novel Software Systems, [code](https://github.com/openproblems-bio/neurips2021_multimodal_topmethods/tree/main/src/predict_modality/methods/novel)**   
Nikolay	Russkikh, Gleb	Ryazantsev, Igor	I


<m-d-header text="Overall - DANCE" h=3 add-hash></m-d-header>

**Michigan State University, [code](https://github.com/openproblems-bio/neurips2021_multimodal_topmethods/tree/main/src/predict_modality/methods/DANCE)**  
Hongzhi Wen, Jiayuan	Ding, Wei	Jin, Xiaoyan	Li, Zhaoheng	Li, Yiqi	Wang, Haoyu	Han, Yanyi	Ding, Xiaochun	Ni, Yu	Lei, Yuying	Xie, Jiliang	Tang

<m-d-header text="Task 2 - Match Modality" h=2 add-hash></m-d-header>

_Given collections of cells in each modality, match profiles that originated from the same cell._


<m-d-header text="Winner in all categories - CLUE" h=3 add-hash></m-d-header>

**Peking University, University of Washington, [code](https://github.com/openproblems-bio/neurips2021_multimodal_topmethods/tree/main/src/match_modality/methods/clue)**  
Zhi-Jie	Cao, Xinming	Tu, Chen-Rui	Xia


<m-d-header text="Task 3 - Joint Embedding" h=2 add-hash></m-d-header>

_Learn a low dimensional embedding of both modalities that preserves biology and removes batch effects._

<m-d-header text="Multiome, pre-trained & CITE, pre-trained - Amateur" h=3 add-hash></m-d-header>

**Stanford University, Tsinghua University, [code](https://github.com/openproblems-bio/neurips2021_multimodal_topmethods/tree/main/src/joint_embedding/methods/jae)**  
Qiao	Liu, Wanwen	Zeng, Chencheng	Xu

<m-d-header text="Multiome, online - Living Systems Lab" h=3 add-hash></m-d-header>

**KAUST, [code](https://github.com/openproblems-bio/neurips2021_multimodal_topmethods/tree/main/src/joint_embedding/methods/lsl_ae)**  
Sumeer	Khan, Robert 	Lehman, Xabier Martinez	De Morentin, Minxing	Pang, Aidyn	Ubingazhibov, David	Gomez-Cabrero, Narsis	Kiani, Jesper	Tegner

<m-d-header text="CITE, online - Dengkw" h=3 add-hash></m-d-header>

**University of Michigan, [code](https://github.com/openproblems-bio/neurips2021_multimodal_topmethods/tree/main/src/joint_embedding/methods/Guanlab-dengkw)**  
Kaiwen Deng
