[
    {
        "task_id": "label_projection", 
        "commit_sha": "a49ae996dc5f34402b1fa5cab9b3a3936126a413", 
        "metric_id": "accuracy", 
        "metric_name": "Accuracy", 
        "metric_description": "The percentage of correctly predicted labels.", 
        "paper_reference": "grandini2020metrics", 
        "maximize": true, 
        "image": "openproblems"
    }, 
    {
        "task_id": "label_projection", 
        "commit_sha": "a49ae996dc5f34402b1fa5cab9b3a3936126a413", 
        "metric_id": "f1", 
        "metric_name": "F1 score",
        "metric_description": "Calculates the F1 score for each label, and find their average weighted by support (the number of true instances for each label). This alters 'macro' to account for label imbalance; it can result in an F-score that is not between precision and recall.", 
        "paper_reference": "grandini2020metrics", 
        "maximize": true, 
        "image": "openproblems"
    }, 
    {
        "task_id": "label_projection", 
        "commit_sha": "a49ae996dc5f34402b1fa5cab9b3a3936126a413", 
        "metric_id": "f1_macro", 
        "metric_name": "Macro F1 score", 
        "metric_description": "Calculates the F1 score for each label, and find their unweighted mean. This does not take label imbalance into account.", 
        "paper_reference": "grandini2020metrics", 
        "maximize": true, 
        "image": "openproblems"
    }
]