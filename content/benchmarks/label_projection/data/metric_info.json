[
    {
        "metric_name": "Accuracy",
        "metric_summary": "Average number of correctly applied labels.",
        "paper_reference": "grandini2020metrics",
        "maximize": true,
        "image": "openproblems",
        "task_id": "label_projection",
        "commit_sha": "6cd9e1e3b3b8549a872b901e84b4c7e3d59ae294",
        "metric_id": "accuracy"
    },
    {
        "metric_name": "F1 score",
        "metric_summary": "The [F1 score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) is a weighted average of the precision and recall over all class labels, where an F1 score reaches its best value at 1 and worst score at 0, where each class contributes to the score relative to its frequency in the dataset.",
        "paper_reference": "grandini2020metrics",
        "maximize": true,
        "image": "openproblems",
        "task_id": "label_projection",
        "commit_sha": "6cd9e1e3b3b8549a872b901e84b4c7e3d59ae294",
        "metric_id": "f1"
    },
    {
        "metric_name": "Macro F1 score",
        "metric_summary": "The macro F1 score is an unweighted F1 score, where each class contributes equally, regardless of its frequency.",
        "paper_reference": "grandini2020metrics",
        "maximize": true,
        "image": "openproblems",
        "task_id": "label_projection",
        "commit_sha": "6cd9e1e3b3b8549a872b901e84b4c7e3d59ae294",
        "metric_id": "f1_macro"
    }
]