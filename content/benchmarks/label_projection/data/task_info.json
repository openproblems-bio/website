{"id": "label_projection", "commit_sha": "ea1a2428e22eb582f4a90c5095bc4ac54e3a4567", "task_name": "Label Projection", "task_summary": "Automated cell type annotation from rich, labeled reference data", "task_description": "\n## The task\n\nA major challenge for integrating single cell datasets is creating matching cell type\nannotations for each cell. One of the most common strategies for annotating cell types\nis referred to as\n[\"cluster-then-annotate\"](https://openproblems.bio/bibliography#kiselev2019challenges) whereby\ncells are aggregated into clusters based on feature similarity and then manually\ncharacterized based on differential gene expression or previously identified marker\ngenes. Recently, methods have emerged to build on this strategy and annotate cells\nusing [known marker genes](https://openproblems.bio/bibliography#pliner2019supervised). However,\nthese strategies pose a difficulty for integrating atlas-scale datasets as the\nparticular annotations may not match.\n\nTo ensure that the cell type labels in newly generated datasets match existing reference\ndatasets, some methods align cells to a previously annotated [reference\ndataset](https://openproblems.bio/bibliography#hou2019scmatch) and then\n_project_ labels from the reference to the new dataset.\n\nHere, we compare methods for annotation based on a reference dataset. The datasets\nconsist of two or more samples of single cell profiles that have been manually annotated\nwith matching labels. These datasets are then split into training and test batches, and\nthe task of each method is to train a cell type classifer on the training set and\nproject those labels onto the test set.\n\n## The metrics\n\nMetrics for label projection aim to characterize how well each classifer correctly\nassigns cell type labels to cells in the test set.\n\n* **Accuracy**: Average number of correctly applied labels.\n* **F1 score**: The [F1\n  score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)\n  is a weighted average of the precision and recall over all class labels, where an F1\n  score reaches its best value at 1 and worst score at 0, where each class contributes\n  to the score relative to its frequency in the dataset.\n* **Macro F1 score**: The macro F1 score is an unweighted F1 score, where each class\n  contributes equally, regardless of its frequency.\n\n", "repo": "openproblems-bio/openproblems"}