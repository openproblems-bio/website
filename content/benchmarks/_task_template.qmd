
```{r}
#| include: false
params <- list(data_dir = "content/benchmarks/label_projection/data")
params <- list(data_dir = "./data")
```


```{r setup}
#| include: false
#| error: true

library(tidyverse)
library(funkyheatmap)
library(kableExtra)

# read task info
task_info <- jsonlite::read_json(paste0(params$data_dir, "/task_info.json"))
task_info$task_description <- task_info$task_description %||% NA_character_

`%|%` <- function(x, y) {
  ifelse(is.na(x), y, x)
}

# read method info
method_info <- jsonlite::read_json(paste0(params$data_dir, "/method_info.json"), simplifyVector = TRUE) %>%
  mutate(
    method_base_name = gsub(" \\(.*", "", method_name)
  )

# read metric info
metric_info <- jsonlite::read_json(paste0(params$data_dir, "/metric_info.json"), simplifyVector = TRUE)

# read dataset info
dataset_info <- jsonlite::read_json(paste0(params$data_dir, "/dataset_info.json"), simplifyVector = TRUE)

# read results
results <- jsonlite::read_json(paste0(params$data_dir, "/results.json"), simplifyVector = TRUE)

# read qc values
qc <- jsonlite::read_json(paste0(params$data_dir, "/quality_control.json"), simplifyVector = TRUE)
```

`r task_info$task_description %|% "Missing 'task_description'"`

## Results

```{r overall_ranking}
#| echo: false
results_long <-
  inner_join(
    results %>%
      unnest(metric_values) %>%
      gather(metric_id, value, any_of(metric_info$metric_id)) %>%
      mutate(value = ifelse(is.na(value), NA_real_, value)) %>%
      select(method_id, dataset_id, metric_id, value),
    results %>%
      unnest(scaled_scores) %>%
      gather(metric_id, score, any_of(metric_info$metric_id)) %>%
      mutate(score = ifelse(is.na(score), NA_real_, score)) %>%
      select(method_id, dataset_id, metric_id, score),
    by = c("method_id", "dataset_id", "metric_id")
  ) %>%
  left_join(method_info %>% select(method_id, is_baseline), "method_id")

overall_ranking <- results_long %>%
  group_by(method_id) %>%
  summarise(mean_score = mean(score)) %>%
  arrange(desc(mean_score))

# order by ranking
results_long$method_id <- factor(results_long$method_id, levels = rev(overall_ranking$method_id))
```

```{r funkyheatmap_data}
#| echo: false
#| message: false
#| warning: false
per_dataset <- results_long %>%
  group_by(method_id, dataset_id) %>%
  summarise(score = mean(score), .groups = "drop") %>%
  mutate(dataset_id = paste0("dataset_", dataset_id)) %>%
  spread(dataset_id, score)
per_metric <- results_long %>%
  group_by(method_id, metric_id) %>%
  summarise(score = mean(score), .groups = "drop") %>%
  mutate(metric_id = paste0("metric_", metric_id)) %>%
  spread(metric_id, score)

summary_all <- 
  method_info %>%
  transmute(
    method_id,
    method_name,
    method_is_baseline = ifelse(is_baseline, "yes", "")
  ) %>%
  left_join(overall_ranking, by = "method_id") %>%
  left_join(per_dataset, by = "method_id") %>%
  left_join(per_metric, by = "method_id") %>%
  arrange(desc(mean_score))
summary_base <- 
  method_info %>%
  transmute(
    method_id,
    method_name = method_base_name,
    method_is_baseline = ifelse(is_baseline, "yes", "")
  ) %>%
  group_by(method_name) %>%
  mutate(method_num_paramsets = as.character(n())) %>%
  left_join(overall_ranking, by = "method_id") %>%
  slice(which.max(mean_score)) %>%
  ungroup() %>%
  arrange(desc(mean_score)) %>%
  left_join(per_dataset, by = "method_id") %>%
  left_join(per_metric, by = "method_id") %>%
  arrange(desc(mean_score))

column_info <- tibble(
  id = colnames(summary_base)[-1],
  name = id %>%
    gsub("^[^_]+_", "", .) %>%
    gsub("_", " ", .) %>%
    str_to_title(),
  group = gsub("_.*", "", id),
  geom = case_when(
    group == "method" ~ "text",
    group == "mean" ~ "bar",
    group %in% c("dataset", "metric") ~ "funkyrect"
  ),
  palette = ifelse(group %in% c("mean", "dataset", "metric"), group, NA_character_),
  options = map2(id, geom, function(id, geom) {
    if (id == "method_name") {
      list(width = 12, hjust = 0)
    } else if (id == "is_baseline") {
      list(width = 1)
    } else if (geom == "bar") {
      list(width = 4)
    } else {
      list()
    }
  })
)

g_base <- funky_heatmap(
  data = summary_base,
  column_info = column_info,
  expand = c(xmax = 3),
  col_annot_offset = 5
)

g_all <- funky_heatmap(
  data = summary_all,
  column_info = column_info %>% filter(id %in% colnames(summary_all)),
  expand = c(xmax = 3),
  col_annot_offset = 5
)
```

```{r}
#| include: false
knitr::opts_chunk$set(
  fig.width = g_base$width,
  fig.height = g_base$height
)
```

```{r summary}
#| echo: false
#| fig-cap: Overview of the results per method. This figures shows the means of the scaled scores per method across all results (group Mean), per dataset (group Dataset) and per metric (group Metric).
g_base
```


## Methods

```{r}
#| echo: false
# show each method just once
method_info_base <- method_info %>% filter(!duplicated(method_base_name), !is_baseline)
lines <- pmap_chr(method_info_base, function(method_base_name, method_description, paper_reference, code_url, ...) {
  name <- method_base_name
  if (!is.na(code_url)) {
    name <- glue::glue("[{name}]({code_url})")
  }
  ref <- if (is.na(paper_reference)) "" else glue::glue(" [@{paper_reference}]")
  summ <- (method_description %|% "Missing 'method_description'") %>% str_replace_all("\\. *$", "")
  glue::glue("* **{name}**{ref}: {summ}.")
})
knitr::asis_output(lines)
```

## Metrics

```{r}
#| echo: false
lines <- pmap_chr(metric_info, function(metric_name, metric_description, paper_reference, ...) {
  ref <- if (is.na(paper_reference)) "" else glue::glue(" [@{paper_reference}]")
  summ <- (metric_description %|% "Missing 'metric_description'") %>% str_replace_all("\\. *$", "")
  glue::glue("* **{metric_name}**{ref}: {summ}.")
})
knitr::asis_output(lines)
```

## Datasets

```{r}
#| echo: false
lines <- pmap_chr(dataset_info, function(dataset_name, dataset_description, data_reference, ...) {
  ref <- if (is.na(data_reference)) "" else glue::glue(" [@{data_reference}]")
  summ <- (dataset_description %|% "Missing 'dataset_description'") %>% str_replace_all("\\. *$", "")
  glue::glue("* **{dataset_name}**{ref}: {summ}.")
})
knitr::asis_output(lines)
```

## Baselines

```{r}
#| echo: false
baselines <- method_info %>% filter(is_baseline)
lines <- pmap_chr(baselines, function(method_base_name, method_description, reference, code_url, ...) {
  summ <- (method_description %|% "Missing 'method_description'") %>% str_replace_all("\\. *$", "")
  glue::glue("* **{method_base_name}**: {summ}.")
})
knitr::asis_output(lines)
```

## Downloads

<a href="data/task_info.json" class="btn btn-secondary">Task info</a>
<a href="data/method_info.json" class="btn btn-secondary">Method info</a>
<a href="data/metric_info.json" class="btn btn-secondary">Metric info</a>
<a href="data/dataset_info.json" class="btn btn-secondary">Dataset info</a>
<a href="data/results.json" class="btn btn-secondary">Results</a>
<a href="data/quality_control.json" class="btn btn-secondary">Quality control</a>

## Details

<details><summary>Overview per parameter set</summary>
```{r}
#| include: false
knitr::opts_chunk$set(
  fig.width = g_all$width,
  fig.height = g_all$height
)
```

```{r summary_defailed}
#| echo: false
#| fig-cap: Overview of the results per method and parameter set. This figures shows the means of the scaled scores per method parameter set across all results (group Mean), per dataset (group Dataset) and per metric (group Metric).
g_all
```
</details>

<details><summary>Quality control</summary>

```{r}
#| echo: false
qc_print <- qc %>% 
  filter(severity > 0) %>%
  mutate(
    test = case_when(
      severity == 0 ~ "✓",
      severity == 1 ~ "✗",
      severity == 2 ~ "✗✗",
      TRUE ~ "✗✗✗"
    ),
    color = ifelse(severity == 0, "green", "red")
  ) %>%
  arrange(desc(severity_value), category, name)

if (nrow(qc_print) > 0) {
  qc_print %>% 
    transmute(
      Category = category,
      Name = name,
      Value = value,
      Condition = code,
      Severity = test
    ) %>%
    kbl(format = "html") %>%
    kable_styling() %>%
    kable_paper() %>%
    column_spec(
      5, 
      color = qc_print$color
    ) %>%
    column_spec(
      1:5,
      tooltip = qc_print$message
    )
} else {
  knitr::asis_output("✓ All checks succeeded!")
}
```

</details>

<details><summary>Visualization of raw results</summary>

```{r}
#| include: false
knitr::opts_chunk$set(
  fig.width = 10,
  fig.height = nrow(method_info) * nrow(metric_info) / 4
)
```

```{r echo=FALSE}
ggplot(results_long %>% arrange(method_id)) +
  geom_vline(aes(xintercept = x), tibble(x = c(0, 1)), linetype = "dashed", alpha = .5, colour = "red") +
  geom_path(aes(score, method_id, group = dataset_id), alpha = .25) +
  geom_point(aes(score, method_id, colour = is_baseline)) +
  facet_wrap(~metric_id, ncol = 1, scales = "free") +
  theme_bw() +
  labs(x = NULL, y = NULL)
```

</details>

## References
