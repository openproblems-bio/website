[
    {
        "method_name": "densMAP (logCP10k)",
        "method_summary": "densMAP is a modification of UMAP that adds an extra cost term in order to preserve information about the relative local density of the data. It is performed on the same inputs as UMAP.",
        "paper_name": "Assessing single-cell transcriptomic variability through density-preserving data visualization",
        "paper_reference": "narayan2021assessing",
        "paper_year": 2021,
        "code_url": "https://github.com/lmcinnes/umap",
        "image": "openproblems-python-extras",
        "is_baseline": false,
        "code_version": null,
        "task_id": "dimensionality_reduction",
        "commit_sha": "6cd9e1e3b3b8549a872b901e84b4c7e3d59ae294",
        "method_id": "densmap_logCP10k"
    },
    {
        "method_name": "densMAP (logCP10k, 1kHVG)",
        "method_summary": "densMAP is a modification of UMAP that adds an extra cost term in order to preserve information about the relative local density of the data. It is performed on the same inputs as UMAP.",
        "paper_name": "Assessing single-cell transcriptomic variability through density-preserving data visualization",
        "paper_reference": "narayan2021assessing",
        "paper_year": 2021,
        "code_url": "https://github.com/lmcinnes/umap",
        "image": "openproblems-python-extras",
        "is_baseline": false,
        "code_version": null,
        "task_id": "dimensionality_reduction",
        "commit_sha": "6cd9e1e3b3b8549a872b901e84b4c7e3d59ae294",
        "method_id": "densmap_logCP10k_1kHVG"
    },
    {
        "method_name": "densMAP PCA (logCP10k)",
        "method_summary": "densMAP is a modification of UMAP that adds an extra cost term in order to preserve information about the relative local density of the data. It is performed on the same inputs as UMAP.",
        "paper_name": "Assessing single-cell transcriptomic variability through density-preserving data visualization",
        "paper_reference": "narayan2021assessing",
        "paper_year": 2021,
        "code_url": "https://github.com/lmcinnes/umap",
        "image": "openproblems-python-extras",
        "is_baseline": false,
        "code_version": null,
        "task_id": "dimensionality_reduction",
        "commit_sha": "6cd9e1e3b3b8549a872b901e84b4c7e3d59ae294",
        "method_id": "densmap_pca_logCP10k"
    },
    {
        "method_name": "densMAP PCA (logCP10k, 1kHVG)",
        "method_summary": "densMAP is a modification of UMAP that adds an extra cost term in order to preserve information about the relative local density of the data. It is performed on the same inputs as UMAP.",
        "paper_name": "Assessing single-cell transcriptomic variability through density-preserving data visualization",
        "paper_reference": "narayan2021assessing",
        "paper_year": 2021,
        "code_url": "https://github.com/lmcinnes/umap",
        "image": "openproblems-python-extras",
        "is_baseline": false,
        "code_version": null,
        "task_id": "dimensionality_reduction",
        "commit_sha": "6cd9e1e3b3b8549a872b901e84b4c7e3d59ae294",
        "method_id": "densmap_pca_logCP10k_1kHVG"
    },
    {
        "method_name": "Diffusion maps",
        "method_summary": "Diffusion maps uses an affinity matrix to describe the similarity between data points, which is then transformed into a graph Laplacian. The eigenvalue-weighted eigenvectors of the graph Laplacian are then used to create the embedding. Diffusion maps is calculated on the logCPM expression matrix.",
        "paper_name": "Diffusion maps",
        "paper_reference": "coifman2006diffusion",
        "paper_year": 2006,
        "code_url": "https://github.com/openproblems-bio/openproblems",
        "image": "openproblems",
        "is_baseline": false,
        "code_version": null,
        "task_id": "dimensionality_reduction",
        "commit_sha": "6cd9e1e3b3b8549a872b901e84b4c7e3d59ae294",
        "method_id": "diffusion_map"
    },
    {
        "method_name": "NeuralEE (CPU) (Default)",
        "method_summary": "NeuralEE is a neural network implementation of elastic embedding. It is a non-linear method that preserves pairwise distances between data points. NeuralEE uses a neural network to optimize an objective function that measures the difference between pairwise distances in the original high-dimensional space and the two-dimensional space. It is computed on both the recommended input from the package authors of 500 HVGs selected from a logged expression matrix (without sequencing depth scaling) and the default logCPM matrix with 1000 HVGs.",
        "paper_name": "NeuralEE: A GPU-Accelerated Elastic Embedding Dimensionality Reduction Method for Visualizing Large-Scale scRNA-Seq Data",
        "paper_reference": "xiong2020neuralee",
        "paper_year": 2020,
        "code_url": "https://github.com/HiBearME/NeuralEE",
        "image": "openproblems-python-pytorch",
        "is_baseline": false,
        "code_version": null,
        "task_id": "dimensionality_reduction",
        "commit_sha": "6cd9e1e3b3b8549a872b901e84b4c7e3d59ae294",
        "method_id": "neuralee_default"
    },
    {
        "method_name": "NeuralEE (CPU) (logCP10k, 1kHVG)",
        "method_summary": "NeuralEE is a neural network implementation of elastic embedding. It is a non-linear method that preserves pairwise distances between data points. NeuralEE uses a neural network to optimize an objective function that measures the difference between pairwise distances in the original high-dimensional space and the two-dimensional space. It is computed on both the recommended input from the package authors of 500 HVGs selected from a logged expression matrix (without sequencing depth scaling) and the default logCPM matrix with 1000 HVGs.",
        "paper_name": "NeuralEE: A GPU-Accelerated Elastic Embedding Dimensionality Reduction Method for Visualizing Large-Scale scRNA-Seq Data",
        "paper_reference": "xiong2020neuralee",
        "paper_year": 2020,
        "code_url": "https://github.com/HiBearME/NeuralEE",
        "image": "openproblems-python-pytorch",
        "is_baseline": false,
        "code_version": null,
        "task_id": "dimensionality_reduction",
        "commit_sha": "6cd9e1e3b3b8549a872b901e84b4c7e3d59ae294",
        "method_id": "neuralee_logCP10k_1kHVG"
    },
    {
        "method_name": "PCA (logCP10k)",
        "method_summary": "PCA or \"Principal Component Analysis\" is a linear method that finds orthogonal directions in the data that capture the most variance. The first two principal components are chosen as the two-dimensional embedding. We select only the first two principal components as the two-dimensional embedding. PCA is calculated on the logCPM expression matrix with and without selecting 1000 HVGs.",
        "paper_name": "On lines and planes of closest fit to systems of points in space",
        "paper_reference": "pearson1901pca",
        "paper_year": 1901,
        "code_url": "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html",
        "image": "openproblems",
        "is_baseline": false,
        "code_version": null,
        "task_id": "dimensionality_reduction",
        "commit_sha": "6cd9e1e3b3b8549a872b901e84b4c7e3d59ae294",
        "method_id": "pca_logCP10k"
    },
    {
        "method_name": "PCA (logCP10k, 1kHVG)",
        "method_summary": "PCA or \"Principal Component Analysis\" is a linear method that finds orthogonal directions in the data that capture the most variance. The first two principal components are chosen as the two-dimensional embedding. We select only the first two principal components as the two-dimensional embedding. PCA is calculated on the logCPM expression matrix with and without selecting 1000 HVGs.",
        "paper_name": "On lines and planes of closest fit to systems of points in space",
        "paper_reference": "pearson1901pca",
        "paper_year": 1901,
        "code_url": "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html",
        "image": "openproblems",
        "is_baseline": false,
        "code_version": null,
        "task_id": "dimensionality_reduction",
        "commit_sha": "6cd9e1e3b3b8549a872b901e84b4c7e3d59ae294",
        "method_id": "pca_logCP10k_1kHVG"
    },
    {
        "method_name": "PHATE (default)",
        "method_summary": "PHATE or \u201cPotential of Heat - diffusion for Affinity - based Transition Embedding\u201d uses the potential of heat diffusion to preserve trajectories in a dataset via a diffusion process. It is an affinity - based method that creates an embedding by finding the dominant eigenvalues of a Markov transition matrix. We evaluate several variants including using the recommended square - root transformed CPM matrix as input, this input with the gamma parameter set to zero and the normal logCPM transformed matrix with and without HVG selection.",
        "paper_name": "Visualizing Structure and Transitions in High-Dimensional Biological Data",
        "paper_reference": "moon2019visualizing",
        "paper_year": 2019,
        "code_url": "https://github.com/KrishnaswamyLab/PHATE/",
        "image": "openproblems-python-extras",
        "is_baseline": false,
        "code_version": null,
        "task_id": "dimensionality_reduction",
        "commit_sha": "6cd9e1e3b3b8549a872b901e84b4c7e3d59ae294",
        "method_id": "phate_default"
    },
    {
        "method_name": "PHATE (logCP10k, 1kHVG)",
        "method_summary": "PHATE or \u201cPotential of Heat - diffusion for Affinity - based Transition Embedding\u201d uses the potential of heat diffusion to preserve trajectories in a dataset via a diffusion process. It is an affinity - based method that creates an embedding by finding the dominant eigenvalues of a Markov transition matrix. We evaluate several variants including using the recommended square - root transformed CPM matrix as input, this input with the gamma parameter set to zero and the normal logCPM transformed matrix with and without HVG selection.",
        "paper_name": "Visualizing Structure and Transitions in High-Dimensional Biological Data",
        "paper_reference": "moon2019visualizing",
        "paper_year": 2019,
        "code_url": "https://github.com/KrishnaswamyLab/PHATE/",
        "image": "openproblems-python-extras",
        "is_baseline": false,
        "code_version": null,
        "task_id": "dimensionality_reduction",
        "commit_sha": "6cd9e1e3b3b8549a872b901e84b4c7e3d59ae294",
        "method_id": "phate_logCP10k"
    },
    {
        "method_name": "PHATE (logCP10k)",
        "method_summary": "PHATE or \u201cPotential of Heat - diffusion for Affinity - based Transition Embedding\u201d uses the potential of heat diffusion to preserve trajectories in a dataset via a diffusion process. It is an affinity - based method that creates an embedding by finding the dominant eigenvalues of a Markov transition matrix. We evaluate several variants including using the recommended square - root transformed CPM matrix as input, this input with the gamma parameter set to zero and the normal logCPM transformed matrix with and without HVG selection.",
        "paper_name": "Visualizing Structure and Transitions in High-Dimensional Biological Data",
        "paper_reference": "moon2019visualizing",
        "paper_year": 2019,
        "code_url": "https://github.com/KrishnaswamyLab/PHATE/",
        "image": "openproblems-python-extras",
        "is_baseline": false,
        "code_version": null,
        "task_id": "dimensionality_reduction",
        "commit_sha": "6cd9e1e3b3b8549a872b901e84b4c7e3d59ae294",
        "method_id": "phate_logCP10k_1kHVG"
    },
    {
        "method_name": "PHATE (gamma=0)",
        "method_summary": "PHATE or \u201cPotential of Heat - diffusion for Affinity - based Transition Embedding\u201d uses the potential of heat diffusion to preserve trajectories in a dataset via a diffusion process. It is an affinity - based method that creates an embedding by finding the dominant eigenvalues of a Markov transition matrix. We evaluate several variants including using the recommended square - root transformed CPM matrix as input, this input with the gamma parameter set to zero and the normal logCPM transformed matrix with and without HVG selection.",
        "paper_name": "Visualizing Structure and Transitions in High-Dimensional Biological Data",
        "paper_reference": "moon2019visualizing",
        "paper_year": 2019,
        "code_url": "https://github.com/KrishnaswamyLab/PHATE/",
        "image": "openproblems-python-extras",
        "is_baseline": false,
        "code_version": null,
        "task_id": "dimensionality_reduction",
        "commit_sha": "6cd9e1e3b3b8549a872b901e84b4c7e3d59ae294",
        "method_id": "phate_sqrt"
    },
    {
        "method_name": "PyMDE Preserve Distances (logCP10k)",
        "method_summary": "PyMDE is a Python implementation of minimum-distortion embedding. It is a non-linear method that preserves distances between cells or neighborhoods in the high-dimensional space. It is computed with options to preserve distances between cells or neighbourhoods and with the logCPM matrix with and without HVG selection as input.",
        "paper_name": "Minimum-Distortion Embedding",
        "paper_reference": "agrawal2021mde",
        "paper_year": 2021,
        "code_url": "https://pymde.org/",
        "image": "openproblems-python-pytorch",
        "is_baseline": false,
        "code_version": null,
        "task_id": "dimensionality_reduction",
        "commit_sha": "6cd9e1e3b3b8549a872b901e84b4c7e3d59ae294",
        "method_id": "pymde_distances_log_cp10k"
    },
    {
        "method_name": "PyMDE Preserve Distances (logCP10k, 1kHVG)",
        "method_summary": "PyMDE is a Python implementation of minimum-distortion embedding. It is a non-linear method that preserves distances between cells or neighborhoods in the high-dimensional space. It is computed with options to preserve distances between cells or neighbourhoods and with the logCPM matrix with and without HVG selection as input.",
        "paper_name": "Minimum-Distortion Embedding",
        "paper_reference": "agrawal2021mde",
        "paper_year": 2021,
        "code_url": "https://pymde.org/",
        "image": "openproblems-python-pytorch",
        "is_baseline": false,
        "code_version": null,
        "task_id": "dimensionality_reduction",
        "commit_sha": "6cd9e1e3b3b8549a872b901e84b4c7e3d59ae294",
        "method_id": "pymde_distances_log_cp10k_hvg"
    },
    {
        "method_name": "PyMDE Preserve Neighbors (logCP10k)",
        "method_summary": "PyMDE is a Python implementation of minimum-distortion embedding. It is a non-linear method that preserves distances between cells or neighborhoods in the high-dimensional space. It is computed with options to preserve distances between cells or neighbourhoods and with the logCPM matrix with and without HVG selection as input.",
        "paper_name": "Minimum-Distortion Embedding",
        "paper_reference": "agrawal2021mde",
        "paper_year": 2021,
        "code_url": "https://pymde.org/",
        "image": "openproblems-python-pytorch",
        "is_baseline": false,
        "code_version": null,
        "task_id": "dimensionality_reduction",
        "commit_sha": "6cd9e1e3b3b8549a872b901e84b4c7e3d59ae294",
        "method_id": "pymde_neighbors_log_cp10k"
    },
    {
        "method_name": "PyMDE Preserve Neighbors (logCP10k, 1kHVG)",
        "method_summary": "PyMDE is a Python implementation of minimum-distortion embedding. It is a non-linear method that preserves distances between cells or neighborhoods in the high-dimensional space. It is computed with options to preserve distances between cells or neighbourhoods and with the logCPM matrix with and without HVG selection as input.",
        "paper_name": "Minimum-Distortion Embedding",
        "paper_reference": "agrawal2021mde",
        "paper_year": 2021,
        "code_url": "https://pymde.org/",
        "image": "openproblems-python-pytorch",
        "is_baseline": false,
        "code_version": null,
        "task_id": "dimensionality_reduction",
        "commit_sha": "6cd9e1e3b3b8549a872b901e84b4c7e3d59ae294",
        "method_id": "pymde_neighbors_log_cp10k_hvg"
    },
    {
        "method_name": "Random Features",
        "method_summary": "Randomly generated two-dimensional coordinates from a normal distribution.",
        "paper_name": "Open Problems for Single Cell Analysis",
        "paper_reference": "openproblems",
        "paper_year": 2022,
        "code_url": "https://github.com/openproblems-bio/openproblems",
        "image": "openproblems",
        "is_baseline": true,
        "code_version": null,
        "task_id": "dimensionality_reduction",
        "commit_sha": "6cd9e1e3b3b8549a872b901e84b4c7e3d59ae294",
        "method_id": "random_features"
    },
    {
        "method_name": "Spectral Features",
        "method_summary": "Use 1000-dimensional diffusions maps as an embedding",
        "paper_name": "Open Problems for Single Cell Analysis",
        "paper_reference": "openproblems",
        "paper_year": 2022,
        "code_url": "https://github.com/openproblems-bio/openproblems",
        "image": "openproblems",
        "is_baseline": true,
        "code_version": null,
        "task_id": "dimensionality_reduction",
        "commit_sha": "6cd9e1e3b3b8549a872b901e84b4c7e3d59ae294",
        "method_id": "spectral_features"
    },
    {
        "method_name": "True Features",
        "method_summary": "Use of the original feature inputs as the 'embedding'.",
        "paper_name": "Open Problems for Single Cell Analysis",
        "paper_reference": "openproblems",
        "paper_year": 2022,
        "code_url": "https://github.com/openproblems-bio/openproblems",
        "image": "openproblems",
        "is_baseline": true,
        "code_version": null,
        "task_id": "dimensionality_reduction",
        "commit_sha": "6cd9e1e3b3b8549a872b901e84b4c7e3d59ae294",
        "method_id": "true_features"
    },
    {
        "method_name": "t-SNE (logCP10k)",
        "method_summary": "t-SNE or t-distributed Stochastic Neighbor Embedding converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. We use the implementation in the scanpy package with the result of PCA on the logCPM expression matrix (with and without HVG selection).",
        "paper_name": "Visualizing Data using t-SNE",
        "paper_reference": "vandermaaten2008visualizing",
        "paper_year": 2008,
        "code_url": "https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE",
        "image": "openproblems-python-extras",
        "is_baseline": false,
        "code_version": null,
        "task_id": "dimensionality_reduction",
        "commit_sha": "6cd9e1e3b3b8549a872b901e84b4c7e3d59ae294",
        "method_id": "tsne_logCP10k"
    },
    {
        "method_name": "t-SNE (logCP10k, 1kHVG)",
        "method_summary": "t-SNE or t-distributed Stochastic Neighbor Embedding converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. We use the implementation in the scanpy package with the result of PCA on the logCPM expression matrix (with and without HVG selection).",
        "paper_name": "Visualizing Data using t-SNE",
        "paper_reference": "vandermaaten2008visualizing",
        "paper_year": 2008,
        "code_url": "https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE",
        "image": "openproblems-python-extras",
        "is_baseline": false,
        "code_version": null,
        "task_id": "dimensionality_reduction",
        "commit_sha": "6cd9e1e3b3b8549a872b901e84b4c7e3d59ae294",
        "method_id": "tsne_logCP10k_1kHVG"
    },
    {
        "method_name": "UMAP (logCP10k)",
        "method_summary": "UMAP or Uniform Manifold Approximation and Projection is an algorithm for dimension reduction based on manifold learning techniques and ideas from topological data analysis. We perform UMAP on the logCPM expression matrix before and after HVG selection and with and without PCA as a pre-processing step.",
        "paper_name": "UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction",
        "paper_reference": "mcinnes2018umap",
        "paper_year": 2018,
        "code_url": "https://github.com/lmcinnes/umap",
        "image": "openproblems",
        "is_baseline": false,
        "code_version": null,
        "task_id": "dimensionality_reduction",
        "commit_sha": "6cd9e1e3b3b8549a872b901e84b4c7e3d59ae294",
        "method_id": "umap_logCP10k"
    },
    {
        "method_name": "UMAP (logCP10k, 1kHVG)",
        "method_summary": "UMAP or Uniform Manifold Approximation and Projection is an algorithm for dimension reduction based on manifold learning techniques and ideas from topological data analysis. We perform UMAP on the logCPM expression matrix before and after HVG selection and with and without PCA as a pre-processing step.",
        "paper_name": "UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction",
        "paper_reference": "mcinnes2018umap",
        "paper_year": 2018,
        "code_url": "https://github.com/lmcinnes/umap",
        "image": "openproblems",
        "is_baseline": false,
        "code_version": null,
        "task_id": "dimensionality_reduction",
        "commit_sha": "6cd9e1e3b3b8549a872b901e84b4c7e3d59ae294",
        "method_id": "umap_logCP10k_1kHVG"
    },
    {
        "method_name": "UMAP PCA (logCP10k)",
        "method_summary": "UMAP or Uniform Manifold Approximation and Projection is an algorithm for dimension reduction based on manifold learning techniques and ideas from topological data analysis. We perform UMAP on the logCPM expression matrix before and after HVG selection and with and without PCA as a pre-processing step.",
        "paper_name": "UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction",
        "paper_reference": "mcinnes2018umap",
        "paper_year": 2018,
        "code_url": "https://github.com/lmcinnes/umap",
        "image": "openproblems",
        "is_baseline": false,
        "code_version": null,
        "task_id": "dimensionality_reduction",
        "commit_sha": "6cd9e1e3b3b8549a872b901e84b4c7e3d59ae294",
        "method_id": "umap_pca_logCP10k"
    },
    {
        "method_name": "UMAP PCA (logCP10k, 1kHVG)",
        "method_summary": "UMAP or Uniform Manifold Approximation and Projection is an algorithm for dimension reduction based on manifold learning techniques and ideas from topological data analysis. We perform UMAP on the logCPM expression matrix before and after HVG selection and with and without PCA as a pre-processing step.",
        "paper_name": "UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction",
        "paper_reference": "mcinnes2018umap",
        "paper_year": 2018,
        "code_url": "https://github.com/lmcinnes/umap",
        "image": "openproblems",
        "is_baseline": false,
        "code_version": null,
        "task_id": "dimensionality_reduction",
        "commit_sha": "6cd9e1e3b3b8549a872b901e84b4c7e3d59ae294",
        "method_id": "umap_pca_logCP10k_1kHVG"
    }
]