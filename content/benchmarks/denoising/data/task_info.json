{
    "task_id": "denoising",
    "commit_sha": "6cd9e1e3b3b8549a872b901e84b4c7e3d59ae294",
    "task_name": "Denoising",
    "task_summary": "Removing noise in sparse single-cell RNA-sequencing count data",
    "task_description": "\nSingle-cell RNA-Seq protocols only detect a fraction of the mRNA molecules present\nin each cell. As a result, the measurements (UMI counts) observed for each gene and each\ncell are associated with generally high levels of technical noise ([Gr\u00fcn et al.,\n2014](https://openproblems.bio/bibliography#grn2014validation)). Denoising describes the\ntask of estimating the true expression level of each gene in each cell. In the\nsingle-cell literature, this task is also referred to as *imputation*, a term which is\ntypically used for missing data problems in statistics. Similar to the use of the terms\n\"dropout\", \"missing data\", and \"technical zeros\", this terminology can create confusion\nabout the underlying measurement process ([Sarkar and Stephens,\n2021](https://openproblems.bio/bibliography#sarkar2021separating)).\n\nA key challenge in evaluating denoising methods is the general lack of a ground truth. A\nrecent benchmark study ([Hou et al.,\n2020](https://openproblems.bio/bibliography#hou2020systematic))\nrelied on flow-sorted datasets, mixture control experiments ([Tian et al.,\n2019](https://openproblems.bio/bibliography#tian2019benchmarking)), and comparisons with\nbulk RNA-Seq data. Since each of these approaches suffers from specific limitations, it\nis difficult to combine these different approaches into a single quantitative measure of\ndenoising accuracy. Here, we instead rely on an approach termed molecular\ncross-validation (MCV), which was specifically developed to quantify denoising accuracy\nin the absence of a ground truth ([Batson et al.,\n2019](https://openproblems.bio/bibliography#batson2019molecular)). In MCV, the observed\nmolecules in a given scRNA-Seq dataset are first partitioned between a *training* and a\n*test* dataset. Next, a denoising method is applied to the training dataset. Finally,\ndenoising accuracy is measured by comparing the result to the test dataset. The authors\nshow that both in theory and in practice, the measured denoising accuracy is\nrepresentative of the accuracy that would be obtained on a ground truth dataset.\n\n",
    "repo": "openproblems-bio/openproblems"
}