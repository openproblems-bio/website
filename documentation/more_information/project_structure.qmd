---
title: Project structure
order: 10
---
The OpenProblems repositories are structured as follows.

![Overview of the OpenProblems repositories, mainly consisting of the main repository and the website.](../images/repositories-thin.svg)

## OpenProblems codebase

In the OpenProblems codebase, the `src` directory contains Viash components that manage various aspects of the project, such as common datasets, tasks, and common processing components. The `target` folder is where artifacts generated from these Viash components are stored, including Dockerized Nextflow modules. The `resources_test` directory contains the test resources required for running unit tests on the Viash components. It is important to note that these test resources are not stored within the git repository. Instead, they are obtained by running the sync test resources component (See ["Getting started"](../contribute/getting_started.qmd)).

The main data flow of the pipeline is shown in @fig-dataflow. The common dataset components create common dataset objects which are used in one or more tasks.

{{< include ../more_information/_project_structure_dataflow.qmd >}}

See the sections below for more information on each of these two pipelines.

### Common dataset components 

Root path: `src/datasets`.

The dataset processing pipeline uses dataset loaders to create raw dataset files (@fig-dataset-workflow). The raw dataset files are then processed to generate common dataset files. Common dataset files are used in one or more tasks.

{{< include _project_structure_dataset_workflow.qmd >}}

- **Dataset loader** (`src/datasets/loaders`): This folder contains components to load and format datasets for various sources.

- **Dataset normalization** (`src/datasets/normalization`): This folder contains various dataset normalization methods.

- **Dataset processors** (`src/datasets/processors`): This folder contains components for processing datasets, such as computing a KNN, PCA, HVG or subsetting.

- **Dataset file and component formats** (`src/datasets/api`): This folder contains specifications for dataset file formats and component interfaces.

- **Resource generation scripts** (`src/common/resources_scripts`): This folder contains scripts for generating the datasets using the dataset loaders, normalization methods and processors.

- **Test resource generation scripts** (`src/common/resources_test_scripts`): This folder contains scripts for generating test resources.


### Task-specific components

Root path: `src/<task_id>`.

Each task should contain a data processor (to transform common datasets into task-specific datasets), methods, control methods (for quality control), and metrics (@fig-task-workflow).

{{< include _project_structure_task_workflow.qmd >}}

- **Process dataset** (`src/<task_id>/process_dataset`): This components processes common components into task-specific dataset objects. In supervised tasks, this component will usually output a solution, a training dataset and a test dataset. In unsupervised tasks, this component usually output a solution and a masked dataset.

- **Control methods** (`src/<task_id>/control_methods`): This folder contains control (or control) components for the task. These components have the same interface as the regular methods but also receive the solution object as input.  It serves as a starting point to test the relative accuracy of new methods in the task, and also as a quality control for the metrics defined in the task. A control method can either be a positive control or a negative control, which set a maximum and minimum threshold for performance, so any new method should perform better than the negative control methods and worse than the positive control method.
    * A *positive control* is a method where the expected results are known, thus resulting in the best possible value for any metric outcome measure. 
    * A *negative control* is a simple, naive, or random method that does not rely on any sophisticated techniques or domain knowledge. 

- **Methods** (`src/<task_id>/methods`): This folder contains method components. Each method component outputs a method output object given the training and test datasets (when applicable).

- **Metrics** (`src/<task_id>/metrics`): This folder contains metric components. Each metric component outputs one or more metric results given a solution object and a method output object.

- **Benchmarking pipeline** (`src/<task_id>/workflows`): This folder contains a Nextflow pipeline defining the benchmarking workflow for this task.

- **File and component formats** (`src/<task_id>/api`): This folder contains specifications for task-specific file formats and component interfaces.

- **Resource generation scripts** (`src/common/resources_scripts`): This folder contains scripts for generating benchmarking resources required for the task.

- **Test resource generation scripts** (`src/common/resources_test_scripts`): This folder contains scripts for generating test resources for the task.


### Common components 

Root path: `src/common`.

This subdirectory contains helper components that help you create new components, unit test other components, or manage datasets.

- **Create component** (`src/common/create_component`): This component helps you to create new method or metric components.

- **Metadata collectors** (`src/common/metadata`): This folder contains scripts to collect and manage metadata information for the different components of the project.

- **Component format checkers** (`src/common/check_component_format`): This folder contains scripts to check and validate the format of different components in the project.

- **File format checkers** (`src/common/check_file_format`): This components validates whether the structure of an AnnData matches a given specification file.

- **Test resource generation scripts** (`src/common/resources_test_scripts`): This folder contains scripts to generate test resources for various components and tasks.

- **Sync test resources** (`src/common/sync_test_resources`): This component synchronizes test resources from S3 to the `test_resources/` directory.

- **Migration helpers** (`src/common/migration`): This folder contains components to speed up and track the code migration from the [OpenProblems v1](https://github.com/openproblems-bio/openproblems) codebase to the [OpenProblems v2](https://github.com/openproblems-bio/openproblems-v2) codebase.


### Resources

Root path: `resources`.

- This folder contains dataset AnnData files used to perform the benchmarks.

### Test resources

Root path: `test_resources`.

- This directory contains the test resources required for running unit tests on the Viash components. It is important to note that these test resources are not stored within the git repository. Instead, they are obtained by running the sync test resources component (See ["Getting started"](../contribute/getting_started.qmd)).

### Artifact folder 

Root path: `target`.

- **Docker artifacts** (`target/docker`): This folder contains Docker-related artifacts generated by Viash.

- **Nextflow artifacts** (`target/nextflow`): This folder contains Nextflow-related artifacts generated by Viash.

### OpenProblems technology stack

- **AnnData**: A Python library for handling annotated data matrices, allowing efficient manipulation and storage of large datasets with metadata. AnnData for R is also used to interact with AnnData objects in R. Thus, the AnnData format is used in both R and Python-based components as a common file format to transfer information between one component to the next.

- **AWS**: Amazon Web Services provides scalable and cost-effective cloud computing and storage. AWS is being used to store datasets, test resources, and run the Nextflow benchmarking pipelines.

- **Docker**: Provides a consistent and reproducible environment for building, packaging, and deploying applications and dependencies across different platforms. Docker images are generated by Viash and stored on [ghcr.io](https://github.com/orgs/openproblems-bio/packages?repo_name=openproblems-v2).

- **GitHub Actions**: A continuous integration and continuous deployment (CI/CD) platform integrated with GitHub. This project uses GitHub Actions to perform continuously [build](https://github.com/openproblems-bio/openproblems-v2/actions/workflows/main-build.yml) and [unit test](https://github.com/openproblems-bio/openproblems-v2/actions/workflows/viash-test.yml) the components in the project.

- **Nextflow**: A workflow management system that simplifies the design, deployment, and execution of complex data processing pipelines, enabling seamless scaling and parallelization. All Nextflow modules are generated by Viash and are stored in the `target/nextflow/` folder in the project releases (and on the `main_build` branch).

- **Python**: A widely used, high-level programming language, offering extensive libraries and packages for data manipulation, analysis, and machine learning. Most of the OpenProblems components are written in Python.

- **R**: A programming language and software environment for statistical computing and graphics, widely used in data analysis and bioinformatics. OpenProblems also offers support for R components.

- **Viash**: A versatile tool for building modular, reusable, and versioned data analysis components, streamlining the development process and improving reproducibility. The components in `src/` are used to build Docker images and Nextflow modules from the Python and R scripts and the acompanying YAML metadata.


## Website

### About

An overview of the project, its purpose, and objectives.

### Benchmark results

The results of the benchmarking experiments for each of the OpenProblems tasks. Each task page follows the same structure.

- **Task description**: A statement of need and the objectives of the task.

- **Summary figure**: A [funkyheatmap](https://funkyheatmap.dynverse.org) visualisation of the performance of methods across the different datasets and metrics.

- **Results table**: A detailed view of the results obtained for each method on each dataset for each metric. Filters allow custom exploration of the results.

- **Metadata information**: Contains essential metadata about the components used in the benchmarking experiments, including datasets, methods, control methods, and metrics.

- **Quality control checks**: Helps to flag potential issues by performing various quality checks on the benchmarking results and components.

- **Raw result JSON files**: Provides access to the raw JSON files containing the complete benchmarking results for further analysis or processing.

### Competition pages

Information about ongoing and past competitions, including rules, deadlines, leaderboards, and documentation.

### Team

The team members involved in the project and their respective roles.

### Documentation

Comprehensive documentation for OpenProblems, including information on how to download and analyse the benchmarking results, how to contribute new components, and how to create new tasks.

### Website technology stack

- **GitHub Actions**: This project uses GitHub Actions for rendering and deploying the main website as well as pull request previews.

- **Quarto**: Quarto is a powerful and flexible static page generator used for rendering this website.

- **Netlify**: This website is hosted on Netlify.
